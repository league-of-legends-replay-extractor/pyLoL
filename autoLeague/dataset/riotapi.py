import requests
import time
import random          # â† ì¶”ê°€
from tqdm import tqdm
from collections import defaultdict     # â† ì¶”ê°€
import csv
import numpy as np
import os
from rich.progress import track  # rich ì§„í–‰ë°” ì„í¬íŠ¸
import pandas as pd
import csv, os, requests, math
import csv, os, requests, math
import multiprocessing as mp
from collections import deque
import os, csv, requests, itertools, time, json
from concurrent.futures import ThreadPoolExecutor, as_completed
from rich.progress import (
    Progress, SpinnerColumn, BarColumn, TextColumn,
    TimeElapsedColumn, TimeRemainingColumn
)
from concurrent.futures import ProcessPoolExecutor, as_completed 
'''ë°ì´í„°ì…‹ ìƒì„±ê¸°, ì›í•˜ëŠ” í‹°ì–´ ì…ë ¥í•´ì£¼ë©´ í•´ë‹¹ í‹°ì–´ëŒ€ì˜ ë¦¬í”Œë ˆì´ë“¤ì„ ì €ì¥í•´ì¤€ë‹¤.'''
class RiotAPI(object):

    '''
        api_key : riot api key
        count : match per each player
    '''
    def __init__(self , api_key):
        self.api_key = api_key


    def getPuuid(self,summonerId):
        try:
            puuid = requests.get(f'https://kr.api.riotgames.com/lol/summoner/v4/summoners/{summonerId}?api_key={self.api_key}').json()["puuid"]
            return puuid
        except:
            return None


    def getSummonerId(self,puuid):
        try:
            summonerId = requests.get(f'https://kr.api.riotgames.com/lol/summoner/v4/summoners/by-puuid/{puuid}?api_key={self.api_key}').json()['id']
            return summonerId
        except:
            return None
    
    # ë¦¬ìŠ¤íŠ¸ì˜ ê° ì›ì†Œê°€ ìˆœì°¨ì ìœ¼ë¡œ ëˆ„ì í•œë‹¤.
    def transfomList(original_list):
        original_list = original_list
        new_list = []

        running_sum = 0
        for value in original_list:
            running_sum += value
            new_list.append(running_sum)

        return new_list

    # ë‘ ë¦¬ìŠ¤íŠ¸ì˜ ì°¨ì´ë¥¼ ë°˜í™˜í•œë‹¤. ( ë¸”ë£¨íŒ€ - ë ˆë“œíŒ€ )
    def subtractList(blue_list1, red_list2):

        result = [a - b for a, b in zip(blue_list1, red_list2)]

        return result

    def getMatchData(self, matchId):
        data = requests.get(f"https://asia.api.riotgames.com/lol/match/v5/matches/{matchId}/timeline?api_key={self.api_key}").json()
        #### ë¸”ë£¨íŒ€ participantId 1 ~ 5 // ë ˆë“œíŒ€ participantId 6 ~ 10
        BLUE_IDs = [1,2,3,4,5]
        RED_IDs = [6,7,8,9,10]
        #### ì—¬ê¸°ëŠ” í”„ë ˆì„ ì¸ë±ìŠ¤ê°€ N ì¼ë•Œ, ì‹¤ì œ ê·¸ í”„ë ˆì„ì´ ë‹´ëŠ” ì‹œê°„ëŒ€ëŠ” N-1 ë¶„ 0ì´ˆë¶€í„° N-1 ë¶„ 59ì´ˆê¹Œì§€ì„
        #### ë”°ë¼ì„œ 5ë¶„ë¶€í„° 15ë¶„ 0ì´ˆê¹Œì§€ì˜ ì •ë³´ëŠ” í”„ë ˆì„ 6ë¶„ë¶€í„° 15ê¹Œì§€ì´ë‹¤.
        frame_5_to_15 = data['info']['frames'][6:16]
        
        # (í•´ë‹¹ ì‹œê°„ëŒ€ì—ì„œ) ê° íŒ€ë³„ í‚¬ ìˆ˜, ì–´ì‹œìŠ¤íŠ¸ ìˆ˜, ëˆ„ì  ëŒ€í˜• ì˜¤ë¸Œì íŠ¸ ì²˜ì¹˜ ìˆ˜ 
        BLUE_KILLS,BLUE_DEATHS,BLUE_ASSISTS,BLUE_OBJECTS,BLUE_GOLDS,BLUE_EXPERIENCES  = [],[],[],[],[],[]   # ì°¸ê³ ë¡œ ì—¬ê¸°ì„œëŠ” ê° ì±”í”¼ì–¸ë“¤ì˜ ì–´ì‹œìŠ¤íŠ¸ í•©
        RED_KILLS,RED_DEATHS,RED_ASSISTS,RED_OBJECTS,RED_GOLDS,RED_EXPERIENCES = [],[],[],[],[],[]



        for frame in frame_5_to_15:

            blue_kills,blue_deaths,blue_assists,blue_objects,blue_golds,blue_experiences  = 0,0,0,0,0,0
            red_kills,red_deaths,red_assists,red_objects,red_golds,red_experiences = 0,0,0,0,0,0


            for event in frame['events']:

                if event['type'] == "CHAMPION_KILL":            # KILL, DEATH, ASSIST
                    if event['killerId'] in BLUE_IDs:
                        blue_kills += 1
                    if event['killerId'] in RED_IDs:
                        red_kills += 1
                    if event['victimId'] in BLUE_IDs:
                        blue_deaths += 1
                    if event['victimId'] in RED_IDs:
                        red_deaths += 1
                    if event.get("assistingParticipantIds") != None:
                        for assistId in event.get("assistingParticipantIds"):
                            if assistId in BLUE_IDs:
                                blue_assists += 1
                            if assistId in RED_IDs:
                                red_assists += 1

                if event['type'] == "ELITE_MONSTER_KILL":       # ELITE MONSTER KILL
                    if event['killerTeamId'] == 100:
                        blue_objects += 1
                    else:
                        red_objects += 1
          

            for index in frame['participantFrames']:
                if int(index)  <= 5:
                    #print(frame['participantFrames'][index]['totalGold'])
                    blue_golds += frame['participantFrames'][index]['totalGold']
                    blue_experiences += frame['participantFrames'][str(index)]['xp']
                else:
                    red_golds += frame['participantFrames'][index]['totalGold']
                    red_experiences += frame['participantFrames'][index]['xp']

              


            BLUE_KILLS.append(blue_kills)
            BLUE_DEATHS.append(blue_deaths)
            BLUE_ASSISTS.append(blue_assists)
            BLUE_OBJECTS.append(blue_objects)
            BLUE_GOLDS.append(blue_golds)
            BLUE_EXPERIENCES.append(blue_experiences)

            # ëˆ„ì  íšŸìˆ˜ë¡œ ì „í™˜
            BLUE_KILLS = self.transfomList(BLUE_KILLS)
            BLUE_DEATHS = self.transfomList(BLUE_DEATHS)
            BLUE_ASSISTS = self.transfomList(BLUE_ASSISTS)
            BLUE_OBJECTS = self.transfomList(BLUE_OBJECTS)
            BLUE_GOLDS = self.transfomList(BLUE_GOLDS)
            BLUE_EXPERIENCES = self.transfomList(BLUE_EXPERIENCES)

            RED_KILLS.append(red_kills)
            RED_DEATHS.append(red_deaths)
            RED_ASSISTS.append(red_assists)
            RED_OBJECTS.append(red_objects)
            RED_GOLDS.append(red_golds)
            RED_EXPERIENCES.append(red_experiences)

            # ëˆ„ì  íšŸìˆ˜ë¡œ ì „í™˜
            RED_KILLS = self.transfomList(RED_KILLS)
            RED_DEATHS = self.transfomList(RED_DEATHS)
            RED_ASSISTS = self.transfomList(RED_ASSISTS)
            RED_OBJECTS = self.transfomList(RED_OBJECTS)
            RED_GOLDS = self.transfomList(RED_GOLDS)
            RED_EXPERIENCES = self.transfomList(RED_EXPERIENCES)


            # íŒ€ ì°¨ì´ ê³„ì‚°

            DIFF_KILLS = self.subtractList(BLUE_KILLS,RED_KILLS)
            DIFF_DEATHS = self.subtractList(BLUE_DEATHS,RED_DEATHS)
            DIFF_ASSISTS = self.subtractList(BLUE_ASSISTS,RED_ASSISTS)
            DIFF_OBJECTS = self.subtractList(BLUE_OBJECTS,RED_OBJECTS)
            DIFF_GOLDS = self.subtractList(BLUE_GOLDS,RED_GOLDS)
            DIFF_EXPERIENCES = self.subtractList(BLUE_EXPERIENCES,RED_EXPERIENCES)

                #ìœ„ ìˆœì„œëŒ€ë¡œ ë¦¬ìŠ¤íŠ¸ ë³‘í•©
        
        allData = []
        '''allData.append(matchId)
        allData.extend(BLUE_KILLS) , allData.extend(BLUE_DEATHS) , allData.extend(BLUE_ASSISTS) , allData.extend(BLUE_OBJECTS) , allData.extend(BLUE_GOLDS) , allData.extend(BLUE_EXPERIENCES)
        allData.extend(RED_KILLS) , allData.extend(RED_DEATHS) , allData.extend(RED_ASSISTS) , allData.extend(RED_OBJECTS) , allData.extend(RED_GOLDS) , allData.extend(RED_EXPERIENCES)'''

        allData.append(DIFF_KILLS) , allData.append(DIFF_DEATHS) , allData.append(DIFF_ASSISTS) , allData.append(DIFF_OBJECTS) , allData.append(DIFF_GOLDS) , allData.append(DIFF_EXPERIENCES)

        # Convert the 2D list to a NumPy array and transpose it to have shape (time_steps, features)
        data_np = np.array(allData).transpose()

        # Add an extra dimension for samples, resulting in shape (samples, time_steps, features)
        data_np = np.expand_dims(data_np, axis=0)
        return data_np
    


    def writeCSVfile(self, matchIds): #ìœ„ì—ì„œ ì¶”ì¶œí•œ ì •ë³´ë“¤ì„ csv íŒŒì¼ë¡œ ì €ì¥í•œë‹¤. ëª¨ë“  matchIdsë“¤ì— ëŒ€í•´ì„œ ì§„í–‰.
        
        features = ['matchID']
        for i in range(10):
            features.append(f'ë¸”ë£¨í‚¬{i}')

        for i in range(10):
            features.append(f'ë¸”ë£¨ë°ìŠ¤{i}')

        for i in range(10):
            features.append(f'ë¸”ë£¨ì–´ì‹œ{i}')

        for i in range(10):
            features.append(f'ë¸”ë£¨ì˜¤ë¸Œì {i}')

        for i in range(10):
            features.append(f'ë ˆë“œí‚¬{i}')

        for i in range(10):
            features.append(f'ë ˆë“œë°ìŠ¤{i}')

        for i in range(10):
            features.append(f'ë ˆë“œì–´ì‹œ{i}')

        for i in range(10):
            features.append(f'ë ˆë“œì˜¤ë¸Œì {i}')


        f = open(rf'riot_api_dataset.csv','a', newline='')
        wr = csv.writer(f)
        wr.writerow(features)
        
        for matchId in tqdm(matchIds):
            row = self.getMatchData(self,matchId)
            
            wr.writerow(row)

        f.close()

        print('csv write complete')


    
    # def save_champnames_from_matches_with_rofl(self, csv_save_folder_dir, csv_name, replay_dir):    
    #     # ë¦¬í”Œë ˆì´ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ (ì˜ˆì‹œ ê²½ë¡œ)
    #     # replay_dir = r'C:\Users\username\Documents\League of Legends\Replays'
    #     # CSVë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
    #     # csv_save_folder_dir = 'data'
    #     def get_champnames_per_match(matchid):
    #         # API í˜¸ì¶œì„ ìœ„í•´ '-'ë¥¼ '_'ë¡œ ë³€í™˜
    #         matchid_api = matchid.replace('-', '_')
    #         url = f'https://asia.api.riotgames.com/lol/match/v5/matches/{matchid_api}?api_key={self.api_key}'
    #         data = requests.get(url).json()
    #         players = data['info']['participants']
    #         champNames = [player['championName'] for player in players]
    #         return champNames
    
    #     if not os.path.exists(csv_save_folder_dir):
    #         os.makedirs(csv_save_folder_dir)

    #     csv_file = os.path.join(csv_save_folder_dir, csv_name)

    #     with open(csv_file, mode='w', newline='', encoding='utf-8') as f:
    #         writer = csv.writer(f)
    #         # í—¤ë” ì‘ì„±: match_idì™€ 10ê°œì˜ ì±”í”¼ì–¸ ì»¬ëŸ¼
    #         writer.writerow(['match_id'] + [f'champion_{i+1}' for i in range(10)])
            
    #         # replay_dir ì•ˆì˜ ëª¨ë“  *.rofl íŒŒì¼ ëª©ë¡ ìƒì„±
    #         replay_files = [filename for filename in os.listdir(replay_dir) if filename.endswith('.rofl')]
            
    #         # rich ì§„í–‰ë°”ë¥¼ ì‚¬ìš©í•´ íŒŒì¼ ëª©ë¡ ìˆœíšŒ
    #         for filename in track(replay_files, description="Processing replays..."):
    #             match_id = os.path.splitext(filename)[0]
    #             try:
    #                 champ_names = get_champnames_per_match(match_id)
    #                 if len(champ_names) == 10:
    #                     writer.writerow([match_id] + champ_names)
    #                 else:
    #                     print(f"ê²½ê¸° {match_id}ì— ì±”í”¼ì–¸ì´ 10ê°œê°€ ì•„ë‹™ë‹ˆë‹¤.")
    #             except Exception as e:
    #                 print(f"{match_id} ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

    def save_champnames_from_matches_with_rofl(
        self,
        csv_save_folder_dir: str,
        csv_name: str,
        replay_dir: str,
        max_workers: int = 8,          # ë™ì‹œì— ëŒë¦´ ìŠ¤ë ˆë“œ ìˆ˜
    ):
        """ë¦¬í”Œë ˆì´(.rofl) íŒŒì¼ë§Œ ìˆì„ ë•Œ:
        íŒŒì¼ëª…ì—ì„œ matchidë¥¼ ì¶”ì¶œ â†’ Riot APIë¡œ ì±”í”¼ì–¸ëª… 10ê°œ ì¡°íšŒ â†’ CSV ì €ì¥
        """
        # â”€â”€ ë‚´ë¶€ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        def fetch_champnames(match_id: str) -> list[str]:
            # API í˜¸ì¶œì„ ìœ„í•´ '-' â†’ '_' ì¹˜í™˜
            matchid_api = match_id.replace("-", "_")
            url = (
                f"https://asia.api.riotgames.com/lol/match/v5/matches/{matchid_api}"
                f"?api_key={self.api_key}"
            )
            data = requests.get(url, timeout=10).json()
            return [p["championName"] for p in data["info"]["participants"]]

        # â”€â”€ ì¶œë ¥ ë””ë ‰í„°ë¦¬ ì¤€ë¹„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if csv_save_folder_dir and not os.path.exists(csv_save_folder_dir):
            os.makedirs(csv_save_folder_dir, exist_ok=True)
        csv_file = os.path.join(csv_save_folder_dir, csv_name)

        # â”€â”€ CSV í—¤ë”(ìµœì´ˆ 1íšŒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if not os.path.exists(csv_file):
            with open(csv_file, "w", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow(["match_id"] + [f"champion_{i+1}" for i in range(10)])

        # â”€â”€ ë¦¬í”Œë ˆì´ íŒŒì¼ ëª©ë¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        replay_files = [
            fn for fn in os.listdir(replay_dir) if fn.lower().endswith(".rofl")
        ]
        match_ids = [os.path.splitext(fn)[0] for fn in replay_files]

        # â”€â”€ ì§„í–‰ ë§‰ëŒ€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        progress_cols = [
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(),
            TextColumn("{task.completed}/{task.total}"),
            TimeElapsedColumn(),
            TimeRemainingColumn(),
        ]
        with Progress(*progress_cols, transient=True) as progress:
            task = progress.add_task("Fetching match dataâ€¦", total=len(match_ids))

            with ThreadPoolExecutor(max_workers=max_workers) as pool:
                futures = {
                    pool.submit(fetch_champnames, mid): mid for mid in match_ids
                }

                # CSVëŠ” í•œ ë²ˆë§Œ ì—´ì–´ì„œ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ê¸°ë¡
                with open(csv_file, "a", newline="", encoding="utf-8") as f:
                    writer = csv.writer(f)

                    for fut in as_completed(futures):
                        mid = futures[fut]
                        try:
                            champs = fut.result()
                            if len(champs) == 10:
                                writer.writerow([mid] + champs)
                            else:
                                progress.console.print(
                                    f"[yellow]âš ï¸  {mid}: ì±”í”¼ì–¸ ê°œìˆ˜ {len(champs)}[/]"
                                )
                        except Exception as e:
                            progress.console.print(
                                f"[red]âœ— {mid} ì²˜ë¦¬ ì‹¤íŒ¨: {e}[/]"
                            )
                        progress.advance(task)





    # def save_champnames_from_matches_without_rofl(self, input_csv_path, output_csv_path):
    #     def get_champnames_per_match(matchid):
    #         url = f'https://asia.api.riotgames.com/lol/match/v5/matches/{matchid}?api_key={self.api_key}'
    #         data = requests.get(url).json()
    #         players = data['info']['participants']
    #         champNames = [player['championName'] for player in players]
    #         return champNames

    #     # ë””ë ‰í† ë¦¬ ê²½ë¡œë§Œ ì¶”ì¶œ
    #     output_csv_dir = os.path.dirname(output_csv_path)

    #     if not os.path.exists(output_csv_dir):
    #         os.makedirs(output_csv_dir)

    #     csv_file = output_csv_path
    #     matchids = pd.read_csv(input_csv_path)['matchid'].tolist()

    #     # í—¤ë”ê°€ ì—†ëŠ” ê²½ìš°ë§Œ í—¤ë” ì‘ì„±
    #     if not os.path.exists(csv_file):
    #         with open(csv_file, mode='w', newline='', encoding='utf-8') as f:
    #             writer = csv.writer(f)
    #             writer.writerow(['match_id'] + [f'champion_{i+1}' for i in range(10)])

    #     # í•˜ë‚˜ì”© append ëª¨ë“œë¡œ ì‘ì„±
    #     for match_id in track(matchids, description="Appending match data..."):
    #         try:
    #             champ_names = get_champnames_per_match(match_id)
    #             if len(champ_names) == 10:
    #                 with open(csv_file, mode='a', newline='', encoding='utf-8') as f:
    #                     writer = csv.writer(f)
    #                     writer.writerow([match_id] + champ_names)
    #             else:
    #                 print(f"ê²½ê¸° {match_id}ì— ì±”í”¼ì–¸ì´ 10ê°œê°€ ì•„ë‹™ë‹ˆë‹¤.")
    #         except Exception as e:
    #             print(f"{match_id} ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
    def save_champnames_from_matches_without_rofl(
        self,
        input_csv_path: str,
        output_csv_path: str,
        max_workers: int = 8,          # ğŸŒŸ ë™ì‹œì— í˜¸ì¶œí•  ìŠ¤ë ˆë“œ ìˆ˜
    ):
        """download_replays.ipynb ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ì—ˆì„ ë•Œ:
        CSVì— ìˆëŠ” matchidë¥¼ ì½ì–´ Riot APIë¡œ ì±”í”¼ì–¸ 10ëª…ì„ ì¡°íšŒí•´ ì €ì¥í•œë‹¤.
        """
        # â”€â”€ ë‚´ë¶€ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        def get_champnames_per_match(matchid: str) -> list[str]:
            url = (
                f"https://asia.api.riotgames.com/lol/match/v5/matches/{matchid}"
                f"?api_key={self.api_key}"
            )
            data = requests.get(url, timeout=10).json()
            return [p["championName"] for p in data["info"]["participants"]]

        # â”€â”€ ì¶œë ¥ ë””ë ‰í„°ë¦¬ ì¤€ë¹„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        out_dir = os.path.dirname(output_csv_path)
        if out_dir and not os.path.exists(out_dir):
            os.makedirs(out_dir, exist_ok=True)

        # â”€â”€ matchid ëª©ë¡ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        matchids = pd.read_csv(input_csv_path)["matchid"].tolist()

        # â”€â”€ ê²°ê³¼ CSV í—¤ë”(ìµœì´ˆ 1íšŒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if not os.path.exists(output_csv_path):
            with open(output_csv_path, "w", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow(["match_id"] + [f"champion_{i+1}" for i in range(10)])

        # â”€â”€ ë³‘ë ¬ ìˆ˜ì§‘ + ì§„í–‰ ë§‰ëŒ€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        progress_cols = [
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(),
            TextColumn("{task.completed}/{task.total}"),
            TimeElapsedColumn(),
            TimeRemainingColumn(),
        ]
        with Progress(*progress_cols, transient=True) as progress:
            task = progress.add_task("Fetching match dataâ€¦", total=len(matchids))

            with ThreadPoolExecutor(max_workers=max_workers) as pool:
                futures = {pool.submit(get_champnames_per_match, mid): mid for mid in matchids}

                # CSVë¥¼ append ëª¨ë“œë¡œ í•œ ë²ˆë§Œ ì—´ì–´ë‘” ë’¤ ìˆœì°¨ì ìœ¼ë¡œ ê¸°ë¡
                with open(output_csv_path, "a", newline="", encoding="utf-8") as f:
                    writer = csv.writer(f)

                    for fut in as_completed(futures):
                        match_id = futures[fut]
                        try:
                            champs = fut.result()
                            if len(champs) == 10:
                                writer.writerow([match_id] + champs)
                            else:
                                progress.console.print(
                                    f"[yellow]âš ï¸  {match_id}: ì±”í”¼ì–¸ ê°œìˆ˜ {len(champs)}[/]"
                                )
                        except Exception as e:
                            progress.console.print(
                                f"[red]âœ— {match_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}[/]"
                            )
                        progress.advance(task)
    @staticmethod
    def filter_matches_by_excluded_champions(csv_in, csv_out, excluded_champions):
        # â‘  CSV ë¡œë“œ
        df = pd.read_csv(csv_in)

        # â‘¡ champion_* ì»¬ëŸ¼ë§Œ ê³¨ë¼ì„œ í•œ í–‰(row) ì•ˆì— EXCLUDED_CHAMPIONSê°€
        #    í•˜ë‚˜ë¼ë„ í¬í•¨ë¼ ìˆìœ¼ë©´ True
        champ_cols = [c for c in df.columns if c.startswith("champion_")]
        has_excluded = df[champ_cols].isin(excluded_champions).any(axis=1)

        # â‘¢ ì¡°ê±´ ë°˜ì „(~) â†’ ì œì™¸ ì±”í”¼ì–¸ì´ **ì—†ëŠ”** ê²½ê¸°ë§Œ ì„ íƒ
        filtered_df = df.loc[~has_excluded, ["match_id"]]

        # â‘£ ì €ì¥
        os.makedirs(os.path.dirname(csv_out), exist_ok=True)
        filtered_df.to_csv(csv_out, index=False)

        print(f"ì „ì²´ {len(df)} ê°œ matchid ì¤‘ì—ì„œ {len(filtered_df)}ê°œì˜ match_id ê°€ {csv_out}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    @staticmethod
    def save_top_matches_by_score(
        avg_score_csv_path: str,
        data_json_path: str,
        output_path: str,
        top_n: int = 6000,
    ):
        """
        avg_score_csv_path ì—ì„œ ìƒìœ„ top_nê°œì˜ match_idë¥¼ avg_score ê¸°ì¤€ìœ¼ë¡œ ì¶”ì¶œí•˜ê³ ,
        data_json_path (kill_events_timeline JSON)ì— ì¡´ì¬í•˜ëŠ” ë§¤ì¹˜ë“¤ë§Œ í•„í„°ë§í•˜ì—¬
        output_pathì— ì €ì¥í•©ë‹ˆë‹¤.
        """
        import csv, json, os

        # â‘  í‰ê·  ì ìˆ˜ CSVì—ì„œ match_id, avg_score ë¡œë“œ
        matches = []
        with open(avg_score_csv_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                m_id = row["match_id"]
                if m_id.startswith("KR_"):
                    m_id = m_id.replace("KR_", "")
                avg_score = float(row["avg_score"])
                matches.append((m_id, avg_score))

        # â‘¡ data.json ë¡œë“œ
        with open(data_json_path, "r", encoding="utf-8") as f:
            original_data = json.load(f)

        orig_inner_data = original_data.get("data", {})
        existing_ids = set(orig_inner_data.keys())

        # â‘¢ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” match_id ì¤‘ì—ì„œ avg_score ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ìƒìœ„ Nê°œ
        matches_in_json = [(m_id, score) for (m_id, score) in matches if m_id in existing_ids]
        matches_in_json.sort(key=lambda x: x[1], reverse=True)
        top_match_ids = [m_id for (m_id, _) in matches_in_json[:top_n]]

        # â‘£ í•„í„°ë§í•˜ì—¬ ì €ì¥
        filtered_dict = {m_id: orig_inner_data[m_id] for m_id in top_match_ids}
        result_obj = {"data": filtered_dict}

        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(result_obj, f, ensure_ascii=False, indent=2)

        print(f"data.json ê¸°ì¤€ ìƒìœ„ {len(top_match_ids)}ê°œ ê²½ê¸° í•„í„°ë§ ì™„ë£Œ.")
        print(f"ê²°ê³¼ íŒŒì¼: {output_path}")


    @staticmethod
    def find_disjoint_matches_combinations(target_matches, max_solutions):
        # target_matches = 7  # ì˜ˆì‹œë¡œ 7ê²½ê¸°ë¥¼ ì„ íƒ (ì›ë˜ ì½”ë“œëŠ” 15ì˜€ìœ¼ë‚˜ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 7ë¡œ í•¨)
        # max_solutions = 3   # ìµœëŒ€ 3ê°œì˜ ì¡°í•©ì„ ì°¾ë„ë¡ ì„¤ì •

        def load_matches(csv_path):
            matches = []
            with open(csv_path, newline='', encoding='utf-8') as f:
                reader = csv.reader(f)
                header = next(reader)  # í—¤ë” ê±´ë„ˆë›°ê¸°
                for row in reader:
                    match_id = row[0]
                    champions = row[1:]
                    matches.append((match_id, set(champions)))
            return matches

        def find_disjoint_matches_all(matches, target, start, used_champions, selected, solutions, max_solutions=3):
            # ëª©í‘œ ê²½ê¸° ìˆ˜(target)ë§Œí¼ ì„ íƒí•˜ë©´ í•˜ë‚˜ì˜ ì¡°í•© ì €ì¥
            if len(selected) == target:
                solutions.append(selected.copy())
                return
            # ë‚¨ì€ ê²½ê¸°ë“¤ ìˆœíšŒ
            for i in range(start, len(matches)):
                # ì´ë¯¸ ì›í•˜ëŠ” ì¡°í•© ìˆ˜ë¥¼ ì°¾ì€ ê²½ìš° ì¢…ë£Œ
                if len(solutions) >= max_solutions:
                    break
                match_id, champs = matches[i]
                # ì„ íƒëœ ê²½ê¸°ë“¤ì˜ ì±”í”¼ì–¸ê³¼ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
                if used_champions & champs:
                    continue
                selected.append(match_id)
                new_used = used_champions.union(champs)
                find_disjoint_matches_all(matches, target, i+1, new_used, selected, solutions, max_solutions)
                selected.pop()
                
        # CSV íŒŒì¼ ê²½ë¡œ
        csv_file = os.path.join('data', 'match_champions.csv')
        matches = load_matches(csv_file)
        solutions = []
        find_disjoint_matches_all(matches, target_matches, 0, set(), [], solutions, max_solutions)

        if solutions:
            print(f"ì±”í”¼ì–¸ì´ ê²¹ì¹˜ì§€ ì•ŠëŠ” {target_matches}ê°œ ê²½ê¸° ì§‘í•© (ì´ {len(solutions)}ê°œ):")
            for sol in solutions:
                print(sol)
        else:
            print("ì¡°ê±´ì— ë§ëŠ” ê²½ê¸° ì§‘í•©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")



    
    @staticmethod
    def find_disjoint_matches_combinations_with_initial(
        target_matches,             # ì›í•˜ëŠ” ìµœì¢… match_id ê°œìˆ˜
        max_solutions,              # ìµœëŒ€ ëª‡ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì¡°í•©ì„ ì°¾ì„ì§€
        initial_matches,            # ì´ë¯¸ ê²¹ì¹˜ì§€ ì•ŠëŠ” match_id ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['KR-123', ...])
        csv_file_path=None          # ë§¤ì¹˜ CSV íŒŒì¼ ê²½ë¡œ(ê¸°ë³¸ê°’ None -> ì˜ˆì‹œ dataí´ë”)
    ):
        if csv_file_path is None:
            csv_file_path = os.path.join('data', 'match_champions.csv')
        

        def load_matches(csv_path):
            """CSVì—ì„œ (match_id, í•´ë‹¹ ë§¤ì¹˜ì— ë“±ì¥í•œ championë“¤ì˜ ì§‘í•©) í˜•íƒœë¡œ ë¡œë“œ"""
            matches = []
            with open(csv_path, newline='', encoding='utf-8') as f:
                reader = csv.reader(f)
                header = next(reader)  # í—¤ë” ê±´ë„ˆë›°ê¸°
                for row in reader:
                    match_id = row[0]
                    champions = row[1:]
                    matches.append((match_id, set(champions)))
            return matches

        def find_disjoint_matches_additional(matches, target, start_index,
                                            used_champions, selected,
                                            solutions, max_solutions):
            """
            ì´ë¯¸ selected ì— ì¼ë¶€ match_idê°€ ë“¤ì–´ê°€ ìˆê³ ,
            used_champions ì—ëŠ” ì´ë•Œê¹Œì§€ ì„ íƒëœ ë§¤ì¹˜ë“¤ì˜ ì±”í”¼ì–¸ì´ ì „ë¶€ ë“¤ì–´ê°€ ìˆë‹¤ê³  ê°€ì •í•œë‹¤.
            
            (matchesëŠ” (match_id, champion_set) ë¦¬ìŠ¤íŠ¸)
            targetì€ 'ìµœì¢…ì ìœ¼ë¡œ ì›í•˜ëŠ” match_idì˜ ì´ ê°œìˆ˜'
            start_indexë¶€í„° íƒìƒ‰ì„ ì‹œì‘í•˜ë©°,
            ê²¹ì¹˜ëŠ” ì±”í”¼ì–¸ì´ ì—†ìœ¼ë©´ match_idë¥¼ ì„ íƒ(selectedì— ì¶”ê°€)í•´ì„œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°„ë‹¤.
            """
            # ë§Œì•½ í˜„ì¬ selected ê°œìˆ˜ê°€ targetì— ë„ë‹¬í–ˆë‹¤ë©´ í•˜ë‚˜ì˜ ì¡°í•©ì„ ì™„ì„±í•œ ê²ƒì´ë¯€ë¡œ ì €ì¥
            if len(selected) == target:
                solutions.append(selected.copy())
                return
            
            for i in range(start_index, len(matches)):
                # ì´ë¯¸ ì›í•˜ëŠ” ì¡°í•© ìˆ˜ë¥¼ ì¶©ë¶„íˆ ì°¾ì•˜ë‹¤ë©´ ì¤‘ë‹¨
                if len(solutions) >= max_solutions:
                    break
                
                match_id, champs = matches[i]
                
                # ì´ë¯¸ ì´ˆê¸° ë¦¬ìŠ¤íŠ¸ì— ë“¤ì–´ê°„ match_idëŠ” ì¤‘ë³µìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ê±´ë„ˆëœ€
                if match_id in selected:
                    continue
                
                # ê²¹ì¹˜ëŠ” ì±”í”¼ì–¸ì´ ìˆëŠ”ì§€ í™•ì¸
                if used_champions & champs:
                    continue

                # ìƒˆ ë§¤ì¹˜ ì„ íƒ
                selected.append(match_id)
                new_used = used_champions.union(champs)
                
                # ë‹¤ìŒ ë§¤ì¹˜ íƒìƒ‰ (i+1ë¶€í„°)
                find_disjoint_matches_additional(matches, target, i+1,
                                                new_used, selected,
                                                solutions, max_solutions)
                
                # ë°±íŠ¸ë˜í‚¹ (ì›ìƒë³µêµ¬)
                selected.pop()

        # CSV ë¡œë“œ
        matches = load_matches(csv_file_path)
        
        # ì´ë¯¸ ì£¼ì–´ì§„ match_idë“¤ì˜ ì±”í”¼ì–¸ ì§‘í•© êµ¬í•˜ê¸°
        used_champions = set()
        for match_id, champ_set in matches:
            if match_id in initial_matches:
                used_champions |= champ_set
        
        # í˜„ì¬ê¹Œì§€ ì„ íƒëœ match_id ê°œìˆ˜
        current_len = len(initial_matches)
        
        # ì´ë¯¸ target_matches ì´ìƒ ë“¤ì–´ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ í•œ ê°€ì§€ í•´ë‹µìœ¼ë¡œ ì²˜ë¦¬
        # (ë§Œì•½ target_matches < current_lenì´ë©´ ë¡œì§ìƒ ë” ì¤„ì¼ ìˆœ ì—†ìœ¼ë‹ˆ ê·¸ëŒ€ë¡œ ë¦¬í„´)
        if current_len >= target_matches:
            return [initial_matches]
        
        # ì¬ê·€ë¥¼ í†µí•´ ì¶”ê°€ë¡œ ë½‘ì•„ì•¼ í•˜ëŠ” ìˆ˜
        missing = target_matches - current_len
        
        solutions = []
        
        # ì´ˆê¸° selectedì— initial_matchesë¥¼ ë„£ì€ ì±„ë¡œ ì‹œì‘
        selected = initial_matches.copy()
        
        # ì¬ê·€ í˜¸ì¶œ
        find_disjoint_matches_additional(matches,
                                        target_matches,
                                        start_index=0,
                                        used_champions=used_champions,
                                        selected=selected,
                                        solutions=solutions,
                                        max_solutions=max_solutions)
        
        return solutions
    
    def find_matches_combinations_with_initial_allow_duplicate_2(
            self,
            distinct_champion_target=170,   # ëª©í‘œ ì„œë¡œ ë‹¤ë¥¸ ì±”í”¼ì–¸ ìˆ˜
            max_solutions=10,               # ìµœëŒ€ ë°˜í™˜í•  í•´ë²• ìˆ˜
            initial_matches=None,           # ì´ë¯¸ ì„ íƒëœ ê²½ê¸° ë¦¬ìŠ¤íŠ¸
            csv_file_path=None              # CSV ê²½ë¡œ
    ):
        if initial_matches is None:
            initial_matches = []
        if csv_file_path is None:
            csv_file_path = os.path.join('data', 'match_champions.csv')

        # â‘  CSV ë¡œë“œ
        def load_matches(csv_path):
            matches = []
            with open(csv_path, newline='', encoding='utf-8') as f:
                reader = csv.reader(f)
                next(reader)
                for row in reader:
                    match_id = row[0]
                    champions = set(row[1:])
                    matches.append((match_id, champions))
            return matches

        # â‘¡ API í˜¸ì¶œ (CSVì— ì—†ì„ ë•Œë§Œ)
        def get_champnames_per_match(matchid):
            url = f'https://asia.api.riotgames.com/lol/match/v5/matches/{matchid}?api_key={self.api_key}'
            data = requests.get(url, timeout=3).json()
            return [p['championName'] for p in data['info']['participants']]

        matches = load_matches(csv_file_path)
        csv_ids = {mid for mid, _ in matches}

        # â‘¢ ì´ˆê¸° used_champions ì¤€ë¹„
        used_champs = set()
        for mid in initial_matches:
            if mid in csv_ids:
                used_champs |= dict(matches)[mid]
            else:
                if not self.api_key:
                    raise ValueError(f"API key required for match {mid}")
                used_champs |= set(get_champnames_per_match(mid))

        # ì´ˆê¸° ì„¸íŠ¸ë§Œìœ¼ë¡œ ëª©í‘œ ë‹¬ì„±?
        if len(used_champs) >= distinct_champion_target:
            print(f"ì´ˆê¸° ì¡°í•©ë§Œìœ¼ë¡œë„ â†’ ê²½ê¸° {len(initial_matches)}, ê³ ìœ  ì±”í”¼ì–¸ {len(used_champs)}")
            return [initial_matches]

        solutions = []
        best_len = float('inf')

        # ìŠ¤íƒ: (start_idx, current_union, selected_matches)
        stack = [(0, used_champs, initial_matches.copy())]

        while stack and len(solutions) < max_solutions:
            start, union_champs, selected = stack.pop()

            # ëª©í‘œ ë‹¬ì„±
            if len(union_champs) >= distinct_champion_target:
                # í•´ë²• ê¸°ë¡
                if len(selected) < best_len:
                    best_len = len(selected)
                    solutions.clear()
                if len(selected) == best_len:
                    solutions.append(selected.copy())
                continue

            # ê°€ì§€ì¹˜ê¸°: ì´ë¯¸ í˜„ì¬ best_lenë³´ë‹¤ í¬ë©´ ë³¼ í•„ìš” ì—†ìŒ
            if len(selected) >= best_len:
                continue

            # ë‹¤ìŒ í›„ë³´
            for i in range(start, len(matches)):
                mid, champs = matches[i]
                if mid in selected:
                    continue
                # ê²¹ì¹˜ëŠ” ì±”í”¼ì–¸ ê°œìˆ˜
                overlap = len(union_champs & champs)
                if overlap > 5:
                    continue  # ì¤‘ë³µì´ 6ê°œ ì´ìƒì¸ ë§¤ì¹˜ëŠ” ìŠ¤í‚µ

                new_union = union_champs | champs
                new_selected = selected + [mid]
                stack.append((i+1, new_union, new_selected))

        # ê²°ê³¼ ì¶œë ¥
        if not solutions:
            print("ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì¡°í•©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        final = []
        for idx, sol in enumerate(solutions, 1):
            # ê³ ìœ  ì±”í”¼ì–¸ ìˆ˜ ê³„ì‚°
            cu = set()
            for m in sol:
                if m in csv_ids:
                    cu |= dict(matches)[m]
                else:
                    cu |= set(get_champnames_per_match(m))
            print(f"[ì†”ë£¨ì…˜ {idx}] ê²½ê¸° ìˆ˜={len(sol)}, ê³ ìœ  ì±”í”¼ì–¸ ìˆ˜={len(cu)} â†’ {sol}")
            final.append(sol)

        return final

    


    def find_matches_combinations_with_initial_allow_duplicate(self,
            distinct_champion_target=170,   # ëª©í‘œ ì„œë¡œ ë‹¤ë¥¸ ì±”í”¼ì–¸ ìˆ˜
            max_solutions=10,               # ìµœëŒ€ ë°˜í™˜í•  í•´ë²• ìˆ˜
            initial_matches=None,           # ì´ë¯¸ ì„ íƒëœ ê²½ê¸° ë¦¬ìŠ¤íŠ¸
            csv_file_path=None              # CSV íŒŒì¼ ê²½ë¡œ
    ):
        import os, csv, requests

        if initial_matches is None:
            initial_matches = []
        if csv_file_path is None:
            csv_file_path = os.path.join('data', 'match_champions.csv')

        # â‘  CSV ë¡œë“œ
        def load_matches(csv_path):
            matches = []
            with open(csv_path, newline='', encoding='utf-8') as f:
                reader = csv.reader(f)
                next(reader)  # header skip
                for row in reader:
                    match_id = row[0]
                    champions = set(row[1:])
                    matches.append((match_id, champions))
            return matches

        # â‘¡ API í˜¸ì¶œ (í•„ìš” ì‹œ)
        def get_champnames_per_match(matchid):
            url = f'https://asia.api.riotgames.com/lol/match/v5/matches/{matchid}?api_key={self.api_key}'
            data = requests.get(url, timeout=3).json()
            return [p['championName'] for p in data['info']['participants']]

        # ë§¤ì¹˜ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
        matches = load_matches(csv_file_path)
        id2champs = {mid: champs for mid, champs in matches}
        csv_ids = set(id2champs.keys())

        # â‘¢ initial_matchesë¡œë¶€í„° used_champions êµ¬ì„±
        used_champions = set()
        for mid in initial_matches:
            if mid in csv_ids:
                used_champions |= id2champs[mid]
            else:
                if not self.api_key:
                    raise ValueError(f"API Key í•„ìš”: CSVì— ì—†ëŠ” match '{mid}' ì •ë³´ ì¡°íšŒìš©")
                used_champions |= set(get_champnames_per_match(mid))

        # ì´ˆê¸°ë§Œìœ¼ë¡œ ëª©í‘œ ë‹¬ì„± ì‹œ
        if len(used_champions) >= distinct_champion_target:
            print(f"ì´ˆê¸°ë§Œìœ¼ë¡œ ì¶©ë¶„: ê²½ê¸° {len(initial_matches)}, ê³ ìœ  ì±”í”¼ì–¸ {len(used_champions)}ê°œ")
            return [initial_matches]

        solutions = []
        best_len = float('inf')
        stack = [(0, used_champions, initial_matches.copy())]

        # â‘£ DFS + ë°±íŠ¸ë˜í‚¹
        while stack and len(solutions) < max_solutions:
            start_index, curr_union, selected = stack.pop()

            # ëª©í‘œ ë‹¬ì„±
            if len(curr_union) >= distinct_champion_target:
                if len(selected) < best_len:
                    best_len = len(selected)
                    solutions.clear()
                if len(selected) == best_len:
                    solutions.append(selected.copy())
                continue

            # ê°€ì§€ì¹˜ê¸°
            if len(selected) >= best_len:
                continue

            # ë‹¤ìŒ í›„ë³´
            for i in range(start_index, len(matches)):
                mid, champs = matches[i]
                if mid in selected:
                    continue
                new_union = curr_union | champs
                new_selected = selected + [mid]
                stack.append((i+1, new_union, new_selected))

        # â‘¤ ê²°ê³¼ ì¶œë ¥(ê²½ê¸° ìˆ˜, ê³ ìœ  ì±”í”¼ì–¸ ìˆ˜) í›„ ë°˜í™˜
        if not solutions:
            print("ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì¡°í•©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []

        for idx, sol in enumerate(solutions, 1):
            # ê³ ìœ  ì±”í”¼ì–¸ ìˆ˜ ê³„ì‚°
            champ_set = set()
            for m in sol:
                if m in csv_ids:
                    champ_set |= id2champs[m]
                else:
                    champ_set |= set(get_champnames_per_match(m))
            print(f"[ì†”ë£¨ì…˜ {idx}] ê²½ê¸° ìˆ˜ = {len(sol)}, ê³ ìœ  ì±”í”¼ì–¸ ìˆ˜ = {len(champ_set)} â†’ {sol}")

        return solutions


    def find_disjoint_matches_combinations_with_initial_ver2(
            self,
            target_matches: int,
            max_solutions: int,
            initial_matches: list[str],
            csv_file_path: str | None = None,
            max_duplicates: int = 0              # â† ìƒˆë¡œ ì¶”ê°€: nê°œê¹Œì§€ ì¤‘ë³µ í—ˆìš©
    ):

        if csv_file_path is None:
            csv_file_path = os.path.join('data', 'match_champions.csv')
        if initial_matches is None:
            initial_matches = []

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CSV ë¡œë“œ
        with open(csv_file_path, newline='', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader)
            matches = [(row[0], set(row[1:])) for row in reader]

        id2champs = {mid: champs for mid, champs in matches}
        csv_ids   = set(id2champs.keys())

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ API helper
        _api_cache: dict[str, set[str]] = {}

        def champs_from_api(matchid: str) -> set[str]:
            if matchid in _api_cache:
                return _api_cache[matchid]
            if not getattr(self, "api_key", None):
                raise ValueError("API keyê°€ í•„ìš”í•©ë‹ˆë‹¤")
            url = f"https://asia.api.riotgames.com/lol/match/v5/matches/{matchid}?api_key={self.api_key}"
            data = requests.get(url, timeout=3).json()
            names = {p["championName"] for p in data["info"]["participants"]}
            _api_cache[matchid] = names
            return names

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì´ˆê¸° ìƒíƒœ
        used = set()
        for mid in initial_matches:
            used |= id2champs[mid] if mid in csv_ids else champs_from_api(mid)

        # ì´ˆê¸° ì¡°í•©ì´ ëª©í‘œ ê²½ê¸° ìˆ˜ ì´ìƒì¸ ê²½ìš°
        if len(initial_matches) >= target_matches:
            print(f"[ì´ˆê¸°] ê²½ê¸° {len(initial_matches)}, ê³ ìœ  ì±”í”¼ì–¸ {len(used)}")
            return [initial_matches]

        # ìƒˆ ì±”í”¼ì–¸ ë§ì´ ì£¼ëŠ” ìˆœì„œë¡œ ì •ë ¬
        sorted_matches = sorted(matches,
                                key=lambda mc: len(mc[1] - used),
                                reverse=True)

        solutions: list[list[str]] = []

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DFS (ì¤‘ë³µ nê°œ í—ˆìš©)
        def dfs(start: int, union: set, chosen: list, dupes_used: int):
            if len(chosen) == target_matches:
                solutions.append(chosen.copy())
                return
            if len(solutions) >= max_solutions:
                return

            for idx in range(start, len(sorted_matches)):
                mid, champs = sorted_matches[idx]
                if mid in chosen:
                    continue

                overlap = union & champs
                new_dupes = len(overlap)
                if dupes_used + new_dupes > max_duplicates:
                    continue  # í—ˆìš© ì¤‘ë³µ ì´ˆê³¼

                chosen.append(mid)
                dfs(idx + 1, union | champs, chosen, dupes_used + new_dupes)
                chosen.pop()

                if len(solutions) >= max_solutions:
                    return

        dfs(0, used, initial_matches.copy(), dupes_used=0)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²°ê³¼
        if not solutions:
            print("ì¡°ê±´ì— ë§ëŠ” ì¡°í•©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []

        for i, combo in enumerate(solutions, 1):
            champ_set = set()
            for m in combo:
                champ_set |= id2champs[m] if m in csv_ids else champs_from_api(m)
            print(f"[ì†”ë£¨ì…˜ {i}] ê²½ê¸° {len(combo)}, ê³ ìœ  ì±”í”¼ì–¸ {len(champ_set)} "
                f"(ì¤‘ë³µ í—ˆìš© {max_duplicates}) â†’ {combo}")

        return solutions


    def find_disjoint_matches_combinations_with_initial_ver3(
            self,
            target_matches: int,
            max_solutions: int,
            initial_matches: list[str] | None = None,
            csv_file_path: str | None = None,
            max_duplicates: int = 0,
            verbose: bool = True,
    ):
        """
        ì¤‘ë³µ ì±”í”¼ì–¸ì„ max_duplicatesëª…ê¹Œì§€ í—ˆìš©í•˜ë©°, target_matchesê°œì˜ ê²½ê¸° ì¡°í•©ì„ ì°¾ëŠ”ë‹¤.
        - initial_matches: ì´ë¯¸ ì„ íƒëœ match_id ë¦¬ìŠ¤íŠ¸
        - max_solutions  : ë°˜í™˜í•  ìµœëŒ€ ì†”ë£¨ì…˜ ìˆ˜
        - verbose=True   : ì§„í–‰ ë¡œê·¸Â·í†µê³„ ì¶œë ¥
        """
        import os, csv, requests, itertools

        if csv_file_path is None:
            csv_file_path = os.path.join("data", "match_champions.csv")
        if initial_matches is None:
            initial_matches = []

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘  CSV ë¡œë“œ + ì „ì²´ ê³ ìœ  ì±”í”¼ì–¸ ìˆ˜
        with open(csv_file_path, newline='', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader)  # header skip
            matches = [(row[0], set(row[1:])) for row in reader]

        all_unique_champs = set().union(*(ch for _, ch in matches))
        if verbose:
            print(f"CSV ì „ì²´ ê²½ê¸° í†µí‹€ì–´ ê³ ìœ  ì±”í”¼ì–¸ ìˆ˜: {len(all_unique_champs)}ëª…\n")

        id2champs = {mid: champs for mid, champs in matches}
        csv_ids   = set(id2champs.keys())

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘¡ API helper (CSVì— ì—†ëŠ” ê²½ê¸° ì²˜ë¦¬)
        _api_cache: dict[str, set[str]] = {}

        def champs_from_api(matchid: str) -> set[str]:
            if matchid in _api_cache:
                return _api_cache[matchid]
            if not getattr(self, "api_key", None):
                raise ValueError("self.api_keyê°€ ì„¤ì •ë¼ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            url = f"https://asia.api.riotgames.com/lol/match/v5/matches/{matchid}?api_key={self.api_key}"
            data = requests.get(url, timeout=3).json()
            names = {p["championName"] for p in data["info"]["participants"]}
            _api_cache[matchid] = names
            return names

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘¢ ì´ˆê¸° ìƒíƒœ
        used = set()
        for mid in initial_matches:
            used |= id2champs[mid] if mid in csv_ids else champs_from_api(mid)

        if len(initial_matches) >= target_matches:
            print(f"[ì´ˆê¸°] ê²½ê¸° {len(initial_matches)}, ê³ ìœ  ì±”í”¼ì–¸ {len(used)}")
            return [initial_matches]

        # ìƒˆ ì±”í”¼ì–¸ ê¸°ì—¬ê°€ í° ìˆœì„œë¡œ ì •ë ¬
        sorted_matches = sorted(
            matches,
            key=lambda mc: len(mc[1] - used),
            reverse=True,
        )

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘£ DFS íƒìƒ‰
        solutions: list[list[str]] = []
        node_counter = itertools.count(1)  # ë°©ë¬¸ ë…¸ë“œ ì¹´ìš´í„°

        def dfs(start: int, union: set, chosen: list, dupes_used: int, depth: int):
            # 1000ê°œ ë…¸ë“œë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
            if verbose and next(node_counter) % 1000 == 0:
                print(f"depth={depth:<2} | íƒìƒ‰ ë…¸ë“œ={next(node_counter)-1:<7} "
                    f"| ì„ íƒ ê²½ê¸°={len(chosen)} | ì¤‘ë³µ ì‚¬ìš©={dupes_used}")

            if len(chosen) == target_matches:
                solutions.append(chosen.copy())
                if verbose:
                    print(f"âœ”ï¸  ì†”ë£¨ì…˜ ë°œê²¬! (ê²½ê¸° {len(chosen)})")
                return
            if len(solutions) >= max_solutions:
                return

            for idx in range(start, len(sorted_matches)):
                mid, champs = sorted_matches[idx]
                if mid in chosen:
                    continue

                overlap_cnt = len(union & champs)
                if dupes_used + overlap_cnt > max_duplicates:
                    continue  # í—ˆìš© ì¤‘ë³µ ì´ˆê³¼

                chosen.append(mid)
                dfs(idx + 1,
                    union | champs,
                    chosen,
                    dupes_used + overlap_cnt,
                    depth + 1)
                chosen.pop()

                if len(solutions) >= max_solutions:
                    return

        dfs(0, used, initial_matches.copy(), dupes_used=0, depth=0)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘¤ ê²°ê³¼ ì¶œë ¥
        if not solutions:
            print("ì¡°ê±´ì— ë§ëŠ” ì¡°í•©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []

        for i, combo in enumerate(solutions, 1):
            champ_set = set()
            for m in combo:
                champ_set |= id2champs[m] if m in csv_ids else champs_from_api(m)
            print(f"\n[ì†”ë£¨ì…˜ {i}] "
                f"ê²½ê¸° {len(combo)}, ê³ ìœ  ì±”í”¼ì–¸ {len(champ_set)} "
                f"(ì¤‘ë³µ í—ˆìš© {max_duplicates})\nâ†’ {combo}")

        return solutions



    def find_disjoint_matches_combinations_with_initial_ver4(
            self,
            target_matches: int,
            max_solutions: int,
            initial_matches: list[str] | None = None,
            csv_file_path: str | None = None,
            max_duplicates: int = 0,
            verbose: bool = True,
    ):
        """
        ì¤‘ë³µ ì±”í”¼ì–¸ max_duplicatesëª…ê¹Œì§€ í—ˆìš©.
        ë°˜í™˜: [(match_id_list, unique_champion_set, coverage_ratio), ...]
        """
        import os, csv, requests, itertools

        if csv_file_path is None:
            csv_file_path = os.path.join("data", "match_champions.csv")
        if initial_matches is None:
            initial_matches = []

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘  CSV ë¡œë“œ + ì „ì²´ ê³ ìœ  ì±”í”¼ì–¸ ìˆ˜
        with open(csv_file_path, newline='', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader)
            matches = [(row[0], set(row[1:])) for row in reader]

        all_unique_champs = set().union(*(ch for _, ch in matches))
        if verbose:
            print(f"CSV ì „ì²´ ê²½ê¸° í†µí‹€ì–´ ê³ ìœ  ì±”í”¼ì–¸ ìˆ˜: {len(all_unique_champs)}ëª…\n")

        id2champs = {mid: champs for mid, champs in matches}
        csv_ids   = set(id2champs.keys())

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘¡ API helper
        _api_cache: dict[str, set[str]] = {}

        def champs_from_api(matchid: str) -> set[str]:
            if matchid in _api_cache:
                return _api_cache[matchid]
            if not getattr(self, "api_key", None):
                raise ValueError("self.api_keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            url = (f"https://asia.api.riotgames.com/lol/match/v5/matches/"
                f"{matchid}?api_key={self.api_key}")
            data = requests.get(url, timeout=3).json()
            names = {p["championName"] for p in data["info"]["participants"]}
            _api_cache[matchid] = names
            return names

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘¢ ì´ˆê¸° ìƒíƒœ
        used = set()
        for mid in initial_matches:
            used |= id2champs[mid] if mid in csv_ids else champs_from_api(mid)

        if len(initial_matches) >= target_matches:
            print(f"[ì´ˆê¸°] ê²½ê¸° {len(initial_matches)}, ê³ ìœ  ì±”í”¼ì–¸ {len(used)}")
            champ_set = used
            coverage = sum(1 for _, ch in matches if ch <= champ_set) / len(matches) * 100
            return [(initial_matches, champ_set, coverage)]

        sorted_matches = sorted(
            matches,
            key=lambda mc: len(mc[1] - used),
            reverse=True,
        )

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘£ DFS
        solutions: list[list[str]] = []
        node_counter = itertools.count(1)

        def dfs(start: int, union: set, chosen: list, dupes_used: int, depth: int):
            if verbose and next(node_counter) % 1000 == 0:
                print(f"depth={depth:<2} | ë…¸ë“œ={next(node_counter)-1:<7} "
                    f"| ì„ íƒ={len(chosen)} | ì¤‘ë³µ={dupes_used}")
            if len(chosen) == target_matches:
                solutions.append(chosen.copy())
                if verbose:
                    print(f"âœ”ï¸  ì†”ë£¨ì…˜ ë°œê²¬! (ê²½ê¸° {len(chosen)})")
                return
            if len(solutions) >= max_solutions:
                return

            for idx in range(start, len(sorted_matches)):
                mid, champs = sorted_matches[idx]
                if mid in chosen:
                    continue
                overlap = len(union & champs)
                if dupes_used + overlap > max_duplicates:
                    continue
                chosen.append(mid)
                dfs(idx+1, union | champs, chosen,
                    dupes_used + overlap, depth+1)
                chosen.pop()
                if len(solutions) >= max_solutions:
                    return

        dfs(0, used, initial_matches.copy(), dupes_used=0, depth=0)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘¤ ê²°ê³¼
        if not solutions:
            print("ì¡°ê±´ì— ë§ëŠ” ì¡°í•©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []

        final_results = []

        for i, combo in enumerate(solutions, 1):
            champ_set = set()
            for m in combo:
                champ_set |= id2champs[m] if m in csv_ids else champs_from_api(m)
            # ì»¤ë²„ë¦¬ì§€: champ_setìœ¼ë¡œ ì™„ì „íˆ í¬í•¨ë˜ëŠ” CSV ê²½ê¸° ë¹„ìœ¨
            covered = sum(1 for _, ch in matches if ch <= champ_set)
            coverage_ratio = covered / len(matches) * 100

            print(f"\n[ì†”ë£¨ì…˜ {i}] "
                  f"ê²½ê¸° {len(combo)}, ê³ ìœ  ì±”í”¼ì–¸ {len(champ_set)} "
                  f"(ì¤‘ë³µ í—ˆìš© {max_duplicates})")
            print(f"  â–¶ CSV ê²½ê¸° ì¤‘ {covered}/{len(matches)}ê°œ "
                  f"({coverage_ratio:.2f}%)ê°€ ì´ ì±”í”„ ì§‘í•©ìœ¼ë¡œ ì»¤ë²„ë©ë‹ˆë‹¤.")
            print(f"  â–¶ match_id ë¦¬ìŠ¤íŠ¸: {combo}")

            # â–² ê²½ê¸°ë³„ë¡œ â€˜ì´ë²ˆ ê²½ê¸°ì—ì„œ ìƒˆë¡­ê²Œ ì¶”ê°€ëœ ì±”í”¼ì–¸â€™ ê³„ì‚°Â·ì¶œë ¥
            print("  â–¶ ê²½ê¸°ë³„ ì‹ ê·œ ì±”í”¼ì–¸:")
            seen = set()
            for mid in combo:
                champs = id2champs[mid] if mid in csv_ids else champs_from_api(mid)
                new_champs = champs - seen          # ì´ë²ˆ ê²½ê¸°ë¡œ ì¸í•´ ìƒˆë¡œ í¬í•¨ë˜ëŠ” ì±”í”„
                seen |= champs
                new_list = ", ".join(sorted(new_champs)) if new_champs else "(ì—†ìŒ)"
                print(f"     - {mid}: {new_list}")

            final_results.append((combo, champ_set, coverage_ratio))

        return final_results



    def calc_coverage_with_initial_matches(
            self,
            initial_matches: list[str],
            csv_file_path: str | None = None,
            verbose: bool = True
    ):
        """
        initial_matches ë¡œë¶€í„° ì–»ì€ ê³ ìœ  ì±”í”¼ì–¸ ì§‘í•©ì´
        csv_file_path ì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  ê²½ê¸°ë“¤ ì¤‘ ì–¼ë§ˆë‚˜ ì»¤ë²„(âŠ†) ë˜ëŠ”ì§€ ë¹„ìœ¨ì„ ë°˜í™˜í•œë‹¤.
        
        ë°˜í™˜ê°’: (unique_champ_set, coverage_ratio, covered_count, total_matches)
        """
        import os, csv, requests

        if csv_file_path is None:
            csv_file_path = os.path.join("data", "match_champions.csv")
        if initial_matches is None:
            initial_matches = []

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘  CSV ë¡œë“œ
        with open(csv_file_path, newline='', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader)  # header skip
            matches = [(row[0], set(row[1:])) for row in reader]

        id2champs = {mid: champs for mid, champs in matches}
        csv_ids   = set(id2champs.keys())

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘¡ API helper (CSVì— ì—†ëŠ” ê²½ê¸°)
        _api_cache: dict[str, set[str]] = {}

        def champs_from_api(matchid: str) -> set[str]:
            if matchid in _api_cache:
                return _api_cache[matchid]
            if not getattr(self, "api_key", None):
                raise ValueError("self.api_keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            url = (f"https://asia.api.riotgames.com/lol/match/v5/matches/"
                f"{matchid}?api_key={self.api_key}")
            data = requests.get(url, timeout=3).json()
            names = {p["championName"] for p in data["info"]["participants"]}
            _api_cache[matchid] = names
            return names

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘¢ initial_matches â†’ champ_set
        champ_set = set()
        for mid in initial_matches:
            champ_set |= id2champs[mid] if mid in csv_ids else champs_from_api(mid)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘£ ì»¤ë²„ë¦¬ì§€ ê³„ì‚°
        total = len(matches)
        covered = sum(1 for _, champs in matches if champs <= champ_set)
        coverage_ratio = covered / total * 100 if total else 0.0

        if verbose:
            print(f"â–¶ ì´ˆê¸° match ìˆ˜          : {len(initial_matches)}")
            print(f"â–¶ ê³ ìœ  ì±”í”¼ì–¸ ìˆ˜         : {len(champ_set)}")
            print(f"â–¶ CSV ê²½ê¸° ì´            : {total}")
            print(f"â–¶ ì™„ì „íˆ ì»¤ë²„ëœ ê²½ê¸° ìˆ˜   : {covered}")
            print(f"â–¶ ì»¤ë²„ë¦¬ì§€               : {coverage_ratio:.2f}%")

        return champ_set, coverage_ratio, covered, total



    def find_best_coverage_with_additional_matches(
            self,
            initial_matches: list[str],      # ê³ ì • match_id ë¦¬ìŠ¤íŠ¸
            add_matches: list[str],          # ì¶”ê°€ í›„ë³´ match_id ë¦¬ìŠ¤íŠ¸
            n: int,                          # add_matches ì¤‘ì—ì„œ ê³ ë¥¼ ê°œìˆ˜
            csv_file_path: str | None = None,
            top_k: int | None = 100,         # í›„ë³´ê°€ ë§ì„ ë•Œ ìƒìœ„ kê°œë§Œ ë¸Œë£¨íŠ¸í¬ìŠ¤ (Noneì´ë©´ ì „ì²´)
    ):
        """
        initial_matchesëŠ” ê³ ì •, add_matches ì¤‘ nê°œë¥¼ ë”í•´
        CSV ê²½ê¸° ì»¤ë²„ë¦¬ì§€ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ì¡°í•©ì„ íƒìƒ‰.
        ë°˜í™˜: (best_combo, coverage_ratio, covered_cnt, total_cnt)
        """
        import os, csv, itertools, requests

        if csv_file_path is None:
            csv_file_path = os.path.join("data", "match_champions.csv")

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CSV ë¡œë“œ
        with open(csv_file_path, newline='', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader)
            csv_matches = [(row[0], set(row[1:])) for row in reader]

        id2champs = {mid: champs for mid, champs in csv_matches}
        csv_ids   = set(id2champs.keys())

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ API helper
        _cache: dict[str, set[str]] = {}

        def champs_api(mid):
            if mid in _cache:
                return _cache[mid]
            url = f"https://asia.api.riotgames.com/lol/match/v5/matches/{mid}?api_key={self.api_key}"
            data = requests.get(url, timeout=3).json()
            names = {p["championName"] for p in data["info"]["participants"]}
            _cache[mid] = names
            return names

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ initial ì±”í”„ ì§‘í•©
        base_champs = set()
        for m in initial_matches:
            base_champs |= id2champs[m] if m in csv_ids else champs_api(m)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ add_matchesì˜ ìƒˆ ì±”í”¼ì–¸ ê¸°ì—¬ ê³„ì‚°
        add_info = []
        for m in add_matches:
            champs = id2champs[m] if m in csv_ids else champs_api(m)
            gain   = len(champs - base_champs)
            add_info.append((gain, m, champs))

        # ê¸°ì—¬ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        add_info.sort(reverse=True)

        # top_kë¡œ ì»· (ì˜µì…˜)
        if top_k is not None and len(add_info) > top_k:
            add_info = add_info[:top_k]

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¡°í•© íƒìƒ‰
        best_combo, best_cov, best_cnt = None, -1.0, 0
        total_csv = len(csv_matches)

        for combo in itertools.combinations(add_info, n):
            combo_ids   = [m for _, m, _ in combo]
            combo_champ = base_champs.union(*(c for _, _, c in combo))

            covered = sum(1 for _, champs in csv_matches if champs <= combo_champ)
            cov_pct = covered / total_csv * 100

            if cov_pct > best_cov:
                best_cov, best_combo, best_cnt = cov_pct, combo_ids, covered

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²°ê³¼ ì¶œë ¥
        if best_combo is None:
            print("ì¡°ê±´ì— ë§ëŠ” ì¡°í•©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        print(f"\nâ–  ì´ˆê¸° {len(initial_matches)}ê²½ê¸° + ì¶”ê°€ {n}ê²½ê¸° ì¡°í•© ê²°ê³¼")
        print(f"  â–¶ ìµœì  match_id ì¡°í•© : {best_combo}")
        print(f"  â–¶ ì»¤ë²„ë¦¬ì§€          : {best_cnt}/{total_csv} "
            f"({best_cov:.2f}%)")
        print(f"  â–¶ ê³ ìœ  ì±”í”¼ì–¸ ìˆ˜     : {len(base_champs.union(*(id2champs[m] if m in csv_ids else champs_api(m)
                                                        for m in best_combo)))}")

        return best_combo, best_cov, best_cnt, total_csv


    def save_summoner_leagueinfo_of_replays(
        self,
        replay_dir: str,
        save_folder: str,
        region: str = "KR",
        queue_type: str = "RANKED_SOLO_5x5",
        max_workers: int = 8,            # ğŸŒŸ ë™ì‹œì— ëŒë¦´ ìŠ¤ë ˆë“œ ìˆ˜
    ):
        """
        replay_dir ì˜ .rofl íŒŒì¼ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ
        ì†Œí™˜ì‚¬ 10ì¸ì˜ ë­í¬ ì •ë³´ë¥¼ JSON(ê²½ê¸°ë³„ 1íŒŒì¼)ë¡œ ë³‘ë ¬ ì €ì¥.
        """
        import os, json, requests

        os.makedirs(save_folder, exist_ok=True)

        rofl_files = [f for f in os.listdir(replay_dir) if f.endswith(".rofl")]
        print(f"ì´ {len(rofl_files)}ê°œì˜ ë¦¬í”Œë ˆì´ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        # â”€â”€ ë‚´ë¶€ í•¨ìˆ˜: í•œ ê²½ê¸° ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        def process_one_match(rofl_name: str):
            match_id_api = os.path.splitext(rofl_name)[0].replace("-", "_")
            json_path = os.path.join(save_folder, f"{match_id_api}.json")
            if os.path.exists(json_path):
                return  # ì´ë¯¸ ìˆìŒ

            # 1) match â†’ puuid 10ê°œ
            url_match = (f"https://asia.api.riotgames.com/lol/match/v5/matches/"
                        f"{match_id_api}?api_key={self.api_key}")
            try:
                match_data = requests.get(url_match, timeout=5).json()
                puuids = match_data.get("metadata", {}).get("participants", [])
                if len(puuids) != 10:
                    return
            except Exception:
                return

            # 2) ê° puuid â†’ ë­í¬ ì •ë³´
            players = []
            for puuid in puuids:
                url_rank = (f"https://kr.api.riotgames.com/lol/league/v4/entries/by-puuid/"
                            f"{puuid}?api_key={self.api_key}")
                try:
                    data = requests.get(url_rank, timeout=5).json()
                except Exception:
                    continue

                solo = next((e for e in data if e.get("queueType") == queue_type), None)
                players.append({
                    "puuid": puuid,
                    "tier":  solo.get("tier", "UNRANKED") if solo else "UNRANKED",
                    "rank":  solo.get("rank", "")         if solo else "",
                    "leaguePoints": solo.get("leaguePoints", 0) if solo else 0,
                    "wins":  solo.get("wins", 0)          if solo else 0,
                    "losses": solo.get("losses", 0)       if solo else 0,
                })

            # 3) JSON ì €ì¥
            with open(json_path, "w", encoding="utf-8") as jf:
                json.dump({"match_id": match_id_api, "players": players},
                        jf, ensure_ascii=False, indent=4)

        # â”€â”€ ì§„í–‰ ë§‰ëŒ€(ë”± í•œ ê°œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        progress_cols = [
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(),
            TextColumn("{task.completed}/{task.total}"),
            TimeElapsedColumn(),
            TimeRemainingColumn(),
        ]
        with Progress(*progress_cols, transient=True) as progress:
            task = progress.add_task("Fetching rank dataâ€¦", total=len(rofl_files))

            with ThreadPoolExecutor(max_workers=max_workers) as pool:
                futures = {pool.submit(process_one_match, r): r for r in rofl_files}

                for _ in as_completed(futures):
                    progress.advance(task)

    def save_summoner_leagueinfo_from_csv(
        self,
        csv_in: str  = "data/match_champ_dict(25.13).csv",
        save_folder: str = "data/match_info",
        region: str = "KR",
        queue_type: str = "RANKED_SOLO_5x5",
        max_workers: int = 4,               # ğŸŒŸ ë™ì‹œì— ëŒë¦´ ìŠ¤ë ˆë“œ ìˆ˜
    ):
        """
        CSVì— ìˆëŠ” match_idë³„ë¡œ ì†Œí™˜ì‚¬ ë­í¬ ì •ë³´ë¥¼ JSON(ê²½ê¸°ë‹¹ 1íŒŒì¼)ë¡œ ì €ì¥í•œë‹¤.
        - CSVì—ëŠ” ë°˜ë“œì‹œ 'match_id' ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•¨.
        """
        import pandas as pd, os, json, requests

        # â”€â”€ ì¶œë ¥ í´ë” ì¤€ë¹„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        os.makedirs(save_folder, exist_ok=True)

        # â”€â”€ match_id ëª©ë¡ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        df = pd.read_csv(csv_in)
        if "match_id" not in df.columns:
            raise ValueError(f"'match_id' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤ â†’ {csv_in}")

        match_ids = [mid for mid in df["match_id"].dropna().unique()
                    if str(mid).startswith(f"{region}_")]
        print(f"ì´ {len(match_ids)}ê°œì˜ match_idë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        # â”€â”€ ë‚´ë¶€ í•¨ìˆ˜: í•œ ê²½ê¸° ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        def process_one_match(match_id_api: str):
            json_path = os.path.join(save_folder, f"{match_id_api}.json")
            if os.path.exists(json_path):        # ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ
                return

            # 1) match â†’ puuid 10ê°œ
            url_match = (f"https://asia.api.riotgames.com/lol/match/v5/matches/"
                        f"{match_id_api}?api_key={self.api_key}")
            try:
                match_data = requests.get(url_match, timeout=5).json()
                puuids = match_data.get("metadata", {}).get("participants", [])
                if len(puuids) != 10:
                    return
            except Exception:
                return

            # 2) ê° puuid â†’ ë­í¬ ì •ë³´
            players = []
            for puuid in puuids:
                url_rank = (f"https://kr.api.riotgames.com/lol/league/v4/entries/by-puuid/"
                            f"{puuid}?api_key={self.api_key}")
                try:
                    data = requests.get(url_rank, timeout=5).json()
                except Exception:
                    continue
                solo = next((e for e in data if e.get("queueType") == queue_type), None)
                players.append({
                    "puuid": puuid,
                    "tier":  solo.get("tier", "UNRANKED") if solo else "UNRANKED",
                    "rank":  solo.get("rank", "")         if solo else "",
                    "leaguePoints": solo.get("leaguePoints", 0) if solo else 0,
                    "wins":  solo.get("wins", 0)          if solo else 0,
                    "losses": solo.get("losses", 0)       if solo else 0,
                })

            # 3) JSON ì €ì¥
            with open(json_path, "w", encoding="utf-8") as jf:
                json.dump({"match_id": match_id_api, "players": players},
                        jf, ensure_ascii=False, indent=4)

        # â”€â”€ ì§„í–‰ ë§‰ëŒ€ ì„¤ì • (í•˜ë‚˜ë§Œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        progress_cols = [
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(),
            TextColumn("{task.completed}/{task.total}"),
            TimeElapsedColumn(),
            TimeRemainingColumn(),
        ]
        with Progress(*progress_cols, transient=True) as progress:
            task = progress.add_task("Fetching rank dataâ€¦", total=len(match_ids))

            with ThreadPoolExecutor(max_workers=max_workers) as pool:
                futures = {pool.submit(process_one_match, mid): mid for mid in match_ids}

                for fut in as_completed(futures):
                    progress.advance(task)       # í•œ ê²½ê¸° ì™„ë£Œë  ë•Œë§ˆë‹¤ 1ì¹¸ ì „ì§„



    def save_match_avg_score_from_json(
        self,
        match_info_dir: str,
        output_csv_path: str,
    ):
        """
        match_info_dir ì•ˆì˜ *.json(ê²½ê¸°ë³„ ì†Œí™˜ì‚¬ ë­í¬ ì •ë³´) â†’ 
        10ì¸ í‰ê·  ì ìˆ˜ë¥¼ ê³„ì‚°í•´ output_csv_pathì— ì €ì¥í•œë‹¤.

        Args:
            match_info_dir (str)  : KR_xxx.json íŒŒì¼ë“¤ì´ ë“¤ì–´ ìˆëŠ” ë””ë ‰í„°ë¦¬
            output_csv_path (str) : ê²°ê³¼ CSV ê²½ë¡œ (í—¤ë”: match_id,avg_score)
        """

        # â”€â”€ â‘  ì ìˆ˜ ë³€í™˜ ê¸°ì¤€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        tier_points = {
            "BRONZE": 0, "SILVER": 400, "GOLD": 800,
            "PLATINUM": 1200, "EMERALD": 1600,
            "DIAMOND": 2000, "MASTER": 2400,
            "GRANDMASTER": 2800, "CHALLENGER": 3200,
        }
        rank_points = {"I": 300, "II": 200, "III": 100, "IV": 0}

        # â”€â”€ â‘¡ ê²°ê³¼ CSV í—¤ë” ì¤€ë¹„ (ì—†ìœ¼ë©´ ìƒì„±) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        file_exists = os.path.exists(output_csv_path)
        os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)
        if not file_exists:
            with open(output_csv_path, "w", newline="", encoding="utf-8") as f:
                csv.writer(f).writerow(["match_id", "avg_score"])

        # â”€â”€ â‘¢ JSON ìˆœíšŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        json_files = [fn for fn in os.listdir(match_info_dir) if fn.endswith(".json")]
        for fn in track(json_files, description="Calculating average scoresâ€¦"):
            path = os.path.join(match_info_dir, fn)
            with open(path, "r", encoding="utf-8") as jf:
                data = json.load(jf)

            match_id = data.get("match_id")
            players  = data.get("players", [])
            if not match_id or len(players) != 10:
                continue  # ì •ë³´ ë¶€ì¡± â†’ ìŠ¤í‚µ

            scores = []
            for p in players:
                tier = str(p.get("tier", "")).upper()
                rank = str(p.get("rank", "")).upper()
                lp   = int(p.get("leaguePoints", 0))

                base  = tier_points.get(tier, 0)
                bonus = rank_points.get(rank, 0)
                scores.append(base + bonus + lp)

            if not scores:
                continue

            avg_score = sum(scores) / len(scores)

            # â”€â”€ â‘£ CSVì— ê¸°ë¡(append) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            with open(output_csv_path, "a", newline="", encoding="utf-8") as f:
                csv.writer(f).writerow([match_id, avg_score])









    # ==========================================================
    # â‘  Timeline ë¶„ì„ìš© í—¬í¼ ë©”ì„œë“œë“¤
    # ==========================================================
    @staticmethod
    def _format_timestamp_ms(ts_ms: int) -> str:
        m, s = divmod(round(ts_ms / 1000), 60)
        return f"{m}:{s:02d}"

    @staticmethod
    def get_matchids(full_matchids: list[str]) -> list[str]:
        """'KR-7650415714' â†’ '7650415714'ë¡œ ë³€í™˜í•´ ì •ë ¬"""
        nums = [mid.replace("KR-", "") for mid in full_matchids if mid.startswith("KR-")]
        nums.sort()
        return nums
    
    @staticmethod
    def get_matchids2(full_matchids: list[str]) -> list[str]:
        """'KR-7650415714' â†’ '7650415714'ë¡œ ë³€í™˜í•´ ì •ë ¬"""
        nums = [mid.replace("KR_", "") for mid in full_matchids if mid.startswith("KR_")]
        nums.sort()
        return nums

    def _get_match_timeline_data(self, match_id_num: str, max_retries: int = 5) -> dict:
        """Riot Timeline API í˜¸ì¶œ (ê°„ë‹¨ rate-limit ëŒ€ì‘ í¬í•¨)"""
        url = (f"https://asia.api.riotgames.com/lol/match/v5/matches/"
               f"KR_{match_id_num}/timeline")
        params = {"api_key": self.api_key}

        for _ in range(max_retries):
            try:
                resp = requests.get(url, params=params)
                if resp.status_code == 429:        # Too Many Requests
                    wait = int(resp.headers.get("Retry-After", "2"))
                    time.sleep(wait)
                    continue
                if 500 <= resp.status_code < 600:  # ì„œë²„ ì˜¤ë¥˜
                    time.sleep(2)
                    continue
                resp.raise_for_status()
                time.sleep(random.uniform(0.1, 0.3))
                return resp.json()
            except requests.exceptions.RequestException:
                time.sleep(2)
        raise ValueError(f"[{match_id_num}] timeline í˜¸ì¶œ ì¬ì‹œë„ ì´ˆê³¼")

    @staticmethod
    def _get_champion_kill_logs(tl_data: dict) -> list[dict]:
        logs = []
        for frame in tl_data["info"]["frames"]:
            for ev in frame.get("events", []):
                if ev.get("type") == "CHAMPION_KILL":
                    logs.append({
                        **ev,
                        "formatted_time": RiotAPI._format_timestamp_ms(ev["timestamp"]),
                        "minute": ev["timestamp"] // 60000
                    })
        return logs

    @staticmethod
    def _bind_kill_event_per_minute(kill_logs: list[dict]) -> dict:
        by_min = defaultdict(list)
        for ev in kill_logs:
            by_min[ev["minute"]].append({
                "formatted_time": ev["formatted_time"],
                "position": ev.get("position")
            })
        return {
            str(m): {"event_count": len(v), "events": v}
            for m, v in by_min.items() if len(v) >= 4
        }

    @staticmethod
    def _get_frequent_kill_minute(kill_per_min: dict, kill_events_threshold: int = 4) -> list[int]:
        return sorted(int(m) for m, info in kill_per_min.items()
                      if info["event_count"] >= kill_events_threshold)

    def _process_one_match(self, matchid_num: str, kill_events_threshold) -> list[int]:
        tl = self._get_match_timeline_data(matchid_num)
        kills = self._get_champion_kill_logs(tl)
        per_min = self._bind_kill_event_per_minute(kills)
        return self._get_frequent_kill_minute(per_min, kill_events_threshold)

    # ==========================================================
    # â‘¡ .json ì €ì¥ ë©”ì„œë“œ (ë©€í‹°-í”„ë¡œì„¸ì‹±)
    # ==========================================================
    def save_kill_events_from_matchlist(
        self,
        matchids: list[str],
        output_json_path: str | None = None,
        kill_events_threshold: int = 4,
        max_workers: int = 2,
    ):
        """
        KR-xxxx í˜•ì‹ match_id ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„
        íƒ€ì„ë¼ì¸ì—ì„œ ë¶„ë‹¹ í‚¬ ì´ë²¤íŠ¸ â‰¥4ì¸ ë¶„(minute)ì„ ì°¾ì•„
        save_dir/data.json ì— ëˆ„ì  ì €ì¥.
        """
 
        # ê¸°ì¡´ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°
        if os.path.exists(output_json_path):
            with open(output_json_path, "r", encoding="utf-8") as f:
                result = json.load(f)
        else:
            result = {"data": {}}

        nums = self.get_matchids2(matchids)
        to_do = [m for m in nums if m not in result["data"]]
        if not to_do:
            print("ëª¨ë“  match_idê°€ ì´ë¯¸ ì²˜ë¦¬ë¨.")
            return

        # ë©€í‹°í”„ë¡œì„¸ì‹±
        from multiprocessing import set_start_method
        try:
            set_start_method("spawn", force=True)
        except RuntimeError:
            pass

        with ProcessPoolExecutor(max_workers=max_workers) as exe:
            fut_map = {exe.submit(self._process_one_match, mid, kill_events_threshold): mid for mid in to_do}

            for fut in track(as_completed(fut_map), total=len(fut_map),
                             description="Processing matchesâ€¦"):
                mid = fut_map[fut]
                try:
                    minutes = fut.result()
                    result["data"][mid] = minutes
                except Exception as e:
                    print(f"{mid} ì‹¤íŒ¨: {e}")

                # ì¤‘ê°„ ì €ì¥
                with open(output_json_path, "w", encoding="utf-8") as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)

        print("âœ… ëª¨ë“  ë§¤ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")


