{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:13px\">\n",
    "\n",
    "### ğŸ‡°ğŸ‡· [KR]\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“Œ ê°œìš”\n",
    "\n",
    "ê° ê²½ê¸°(`matchid`)ì— **Pickëœ ì±”í”¼ì–¸ ëª©ë¡ì„ ì €ì¥**í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.\n",
    "\n",
    "- `.rofl` ë¦¬í”Œë ˆì´ íŒŒì¼ì´ ì•„ë‹Œ  \n",
    "- **CSV íŒŒì¼**ì— ìˆëŠ” `matchid` ëª©ë¡ì„ ì½ì–´  \n",
    "- ê° ê²½ê¸°ì— **ì°¸ì—¬í•œ ì±”í”¼ì–¸ ì´ë¦„ë“¤**ì„ ì¶”ì¶œí•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ ìˆëŠ” ë¦¬í”Œë ˆì´ ê°¯ìˆ˜ :  6893\n"
     ]
    }
   ],
   "source": [
    "import os, json, csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from autoLeague.replays.scraper import ReplayScraper\n",
    "from autoLeague.dataset.riotapi import RiotAPI\n",
    "\n",
    "load_dotenv()  \n",
    "API_KEY = os.environ[\"API_KEY\"]\n",
    "\n",
    "# â”€â”€ ê³µí†µ ê²½ë¡œ ì„¤ì •(Set common base path) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "USER_HOME = Path(os.environ[\"USERPROFILE\"])    # C:\\Users\\<ê³„ì •ëª…>\n",
    "\n",
    "GAME_DIR        = Path(r\"C:\\Riot Games\\League of Legends\\Game\")\n",
    "REPLAY_DIR      = str(USER_HOME / \"Documents\" / \"League of Legends\" / \"Replays\")\n",
    "SAVE_DIR        = str(USER_HOME / \"Desktop\" / \"storage\")\n",
    "SCRAPER_DIR     = str(USER_HOME / \"Desktop\" / \"pyLoL\" / \"pyLoL\" / \"autoLeague\" / \"replays\")\n",
    "MATCH_INFO_DIR  = str(USER_HOME / \"Desktop\" / \"pyLoL\" / \"data\" / \"match_info\")\n",
    "\n",
    "ra = RiotAPI(api_key=API_KEY)\n",
    "rs = ReplayScraper(game_dir=GAME_DIR, replay_dir=REPLAY_DIR, save_dir=SAVE_DIR, scraper_dir=SCRAPER_DIR, replay_speed=27, region=\"KR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:13px\">\n",
    "\n",
    "### ğŸ‡°ğŸ‡· [KR]\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“Œ ê°œìš”\n",
    "\n",
    "ì´ì „ ë‹¨ê³„ì—ì„œ ì €ì¥í•œ **ì—¬ëŸ¬ ë¦¬ê·¸ì˜ `matchid` ëª©ë¡ë“¤ì„ í•˜ë‚˜ì˜ CSV íŒŒì¼ë¡œ ë³‘í•©**í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.\n",
    "\n",
    "- ë‹¤ì–‘í•œ ë¦¬ê·¸ì—ì„œ ìˆ˜ì§‘í•œ `matchid` CSVë“¤ì„  \n",
    "- **í•˜ë‚˜ì˜ í†µí•© CSV íŒŒì¼ë¡œ ì •ë¦¬**í•©ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ‡ºğŸ‡¸ [EN]\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“Œ Overview\n",
    "\n",
    "This step **merges the `matchid` lists collected from multiple leagues into a single CSV file**.\n",
    "\n",
    "- Combines `matchid` CSV files from various leagues  \n",
    "- into **one consolidated CSV file**.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸(List of file paths)\n",
    "file_paths = [\n",
    "    \"matchids/matchids_CHALLENGER_I_patch25_13.csv\",\n",
    "    \"matchids/matchids_GRANDMASTER_I_patch25_13.csv\"\n",
    "]\n",
    "\n",
    "# ëª¨ë“  íŒŒì¼ ì½ì–´ì„œ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°(Read all files and merge into one DataFrame)\n",
    "df_list = [pd.read_csv(path) for path in file_paths]\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# ì¤‘ë³µ ì œê±° : match_id ì»¬ëŸ¼ ê¸°ì¤€ (Remove duplicates (based on the 'match_id' column))\n",
    "if 'match_id' in merged_df.columns:\n",
    "    merged_df = merged_df.drop_duplicates(subset='match_id')\n",
    "else:\n",
    "    merged_df = merged_df.drop_duplicates()\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥(Save the result)\n",
    "merged_df.to_csv(\"data/match_champions(25.13).csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:13px\">\n",
    "\n",
    "### ğŸ‡°ğŸ‡· [KR]\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ”€ OPTION 1: `[2] download_replays.ipynb` ë‹¨ê³„ë¥¼ ê±´ë„ˆë›´ ê²½ìš°\n",
    "\n",
    "- `.rofl` ë¦¬í”Œë ˆì´ íŒŒì¼ì´ ì•„ë‹Œ  \n",
    "- **CSV íŒŒì¼ì— ìˆëŠ” `matchid` ëª©ë¡ì„ ì½ì–´**,  \n",
    "- ê° ê²½ê¸°ì— **ì°¸ì—¬í•œ ì±”í”¼ì–¸ ì´ë¦„ë“¤ì„ ì €ì¥**í•©ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ‡ºğŸ‡¸ [EN]\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ”€ OPTION 1: If you skipped step `[2] download_replays.ipynb`\n",
    "\n",
    "- Instead of using `.rofl` replay files,  \n",
    "- this step **reads `matchid` values from a CSV file** and  \n",
    "- **stores the names of champions who participated** in each match.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.save_champnames_from_matches_without_rofl(input_csv_path='data/match_champions(25.13).csv',\n",
    "                                             output_csv_path='data/match_champions_dict(25.13).csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:13px\">\n",
    "\n",
    "### ğŸ‡°ğŸ‡· [KR]\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ”€ OPTION 2: `[2] download_replays.ipynb` ë‹¨ê³„ë¥¼ ì™„ë£Œí•œ ê²½ìš°\n",
    "\n",
    "- `.rofl` ë¦¬í”Œë ˆì´ íŒŒì¼ë“¤ì„ **ì§ì ‘ ì½ì–´ì„œ**,  \n",
    "- ê° ê²½ê¸°ì— **ì°¸ì—¬í•œ ì±”í”¼ì–¸ ì´ë¦„ë“¤ì„ ì €ì¥**í•©ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ‡ºğŸ‡¸ [EN]\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ”€ OPTION 2: If you completed step `[2] download_replays.ipynb`\n",
    "\n",
    "- Reads the `.rofl` replay files directly  \n",
    "- and **records the champion names** for each match.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.save_champnames_from_matches_with_rofl('data', 'match_champions_dict(25.13).csv', REPLAY_DIR) \n",
    "# ë¦¬í”Œë ˆì´ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ (ì˜ˆì‹œ ê²½ë¡œ) : replay_dir = r'C:\\Users\\username\\Documents\\League of Legends\\Replays'\n",
    "# CSVë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ : csv_save_folder_dir = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:13px\">\n",
    "\n",
    "### ğŸ‡°ğŸ‡· [KR]\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§¹ STEP 2: íƒì§€ ë¶ˆê°€ ë˜ëŠ” ì¸ê²Œì„ ì´ˆìƒí™”ê°€ ì¦ì€ ì±”í”¼ì–¸ ì œì™¸\n",
    "\n",
    "- YOLO-11 ê¸°ë°˜ **íƒì§€ ëª¨ë¸ì´ ì œëŒ€ë¡œ ì¸ì‹í•˜ì§€ ëª»í•˜ëŠ” ì±”í”¼ì–¸ë“¤**  \n",
    "  (â†’ í•™ìŠµ ë°ì´í„° ë¬¸ì œ)  \n",
    "- ë˜ëŠ” **íŒ¨ì‹œë¸Œ / ìŠ¤í‚¬ë¡œ ì´ˆìƒí™”ê°€ ìì£¼ ë°”ë€ŒëŠ” ì±”í”¼ì–¸ë“¤**  \n",
    "  (â†’ ì¸ê²Œì„ êµ¬ì¡° ë¬¸ì œ)  \n",
    "ì„ **ë°ì´í„°ì…‹ì—ì„œ ì œê±°í•˜ëŠ” ë‹¨ê³„**ì…ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ‡ºğŸ‡¸ [EN]\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§¹ STEP 2: Filtering out problematic champion classes\n",
    "\n",
    "- Excludes champions that the **YOLO-11â€“based model cannot reliably detect**  \n",
    "  (due to training data limitations),  \n",
    "- and champions whose **in-game portraits frequently change**  \n",
    "  (due to passives or skill transformations).  \n",
    "This step filters them **out of the dataset**.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ì„¤ì •ê°’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSV_IN  = \"data/match_champions_dict(25.13).csv\"   # ì›ë³¸ CSV \n",
    "CSV_OUT = \"data/filtered_matchids(25.13).csv\"              # ê²°ê³¼ ì €ì¥ ê²½ë¡œ\n",
    "EXCLUDED_CHAMPIONS = {\"Neeko\", \"Viego\", \"Yuumi\", \"Amumu\",\n",
    "                      \"Kayle\", \"Nilah\", \"Shyvana\"}  # ì œì™¸ ëŒ€ìƒ ì±”í”¼ì–¸\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "ra.filter_matches_by_excluded_champions(CSV_IN, CSV_OUT, EXCLUDED_CHAMPIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:13px\">\n",
    "\n",
    "### ğŸ‡°ğŸ‡· [KR]\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ¯ STEP 3 (ì„ íƒ): 10ëª… í‰ê·  ì ìˆ˜ ê¸°ë°˜ ìƒìœ„ ê²½ê¸°ë§Œ ì¶”ì¶œ\n",
    "\n",
    "- **ìœ ì € 10ëª…ì˜ í‰ê·  ì ìˆ˜ ê¸°ì¤€**ìœ¼ë¡œ  \n",
    "- **ìƒìœ„ Nê°œ ê²½ê¸°ë§Œ í•„í„°ë§**í•˜ê³  ì‹¶ë‹¤ë©´  \n",
    "- ì´ ë‹¨ê³„ë¥¼ ì ìš©í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ‡ºğŸ‡¸ [EN]\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ¯ STEP 3 (Optional): Filter top N matches by average player score\n",
    "\n",
    "- If you want to **select only the top N matches**  \n",
    "- based on the **average score of all 10 players**,  \n",
    "- you can **apply this optional filtering step**.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH_AVGSCR_CSV_PATH = \"data/match_avg_score(25.13).csv\"\n",
    "\n",
    "### [1] match_champ_dict CSV íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ì‚¬ìš© > match_info í´ë”ì— ê°œë³„ì ìœ¼ë¡œ ì €ì¥.\n",
    "ra.save_summoner_leagueinfo_from_csv(csv_in=CSV_IN, save_folder=MATCH_INFO_DIR, region=\"KR\", queue_type=\"RANKED_SOLO_5x5\")# ê¸°ë³¸ê°’queue_type=\"RANKED_SOLO_5x5\"\n",
    "\n",
    "### match_champ_dict CSV íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ì‚¬ìš©\n",
    "# ra.save_summoner_leagueinfo_of_replays(replay_dir = REPLAY_DIR, save_folder = MATCH_INFO_DIR,region=\"KR\")# ê¸°ë³¸ê°’queue_type=\"RANKED_SOLO_5x5\"\n",
    "\n",
    "### [2] ê° ê²½ê¸° ìœ ì € 10ëª…ì˜ í‰ê·  ì ìˆ˜ ê³„ì‚°í•˜ì—¬ csv ì— ì €ì¥\n",
    "### Calculate the average score of the 10 players in each match and save the results to a CSV file.\n",
    "\n",
    "ra.save_match_avg_score_from_json(match_info_dir=MATCH_INFO_DIR, output_csv_path=MATCH_AVGSCR_CSV_PATH )\n",
    "matchid_list = pd.read_csv(MATCH_AVGSCR_CSV_PATH, usecols=[\"match_id\"]).values.flatten().tolist()\n",
    "\n",
    "### [3] ê° ê²½ê¸°ì—ì„œ ê° ì‹œê°„ëŒ€(ë¶„ ë‹¨ìœ„)ë³„ë¡œ í‚¬ ì´ë²¤íŠ¸ê°€ threshold ì´ìƒì¸ ì‹œê°„ëŒ€ë§Œë§Œ ì¶”ì¶œí•˜ì—¬ json íŒŒì¼ë¡œ ì €ì¥\n",
    "ra.save_kill_events_from_matchlist(matchids=matchid_list, output_json_path=\"data/kill_events_timeline_from_all_matches.json\", kill_events_threshold=4, max_workers=4) #max_workersëŠ” ë³‘ë ¬ ì²˜ë¦¬í•  í”„ë¡œì„¸ìŠ¤ ìˆ˜\n",
    "\n",
    "\n",
    "### [4] â”€â”€ ë¶„(minute) 10 ì´ìƒë§Œ ë‚¨ê¸°ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# â”€â”€ ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "INPUT_JSON  = \"data/kill_events_timeline_from_all_matches.json\"\n",
    "OUTPUT_JSON = \"data/kill_events_timeline_filtered_10up.json\"\n",
    "\n",
    "# â”€â”€ JSON ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)          # {\"data\": {match_id: [minutes...] }}\n",
    "\n",
    "# â”€â”€ ë¶„(minute) 10 ì´ìƒë§Œ ë‚¨ê¸°ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "filtered = {\n",
    "    mid: [m for m in minutes if m >= 10]\n",
    "    for mid, minutes in data.get(\"data\", {}).items()\n",
    "    if any(m >= 10 for m in minutes)       # 10 ì´ìƒì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ í†µì§¸ë¡œ ì œê±°\n",
    "}\n",
    "\n",
    "# â”€â”€ ê²°ê³¼ ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "os.makedirs(os.path.dirname(OUTPUT_JSON), exist_ok=True)\n",
    "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"data\": filtered}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"ì €ì¥ ì™„ë£Œ â†’ {OUTPUT_JSON}  (ê²½ê¸° ìˆ˜: {len(filtered)})\")\n",
    "\n",
    "### [5] ìƒìœ„ Nê°œ ê²½ê¸°ë§Œ ë‚¨ë„ë¡ í•„í„°ë§\n",
    "TOP_KILL_EVENTS_JSON = \"data/top_kill_events_timeline_filtered_10up.json\"\n",
    "ra.save_top_matches_by_score(\n",
    "    avg_score_csv_path=MATCH_AVGSCR_CSV_PATH,\n",
    "    data_json_path=OUTPUT_JSON,\n",
    "    output_path=TOP_KILL_EVENTS_JSON,\n",
    "    top_n=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. ë‘ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\n",
      "data/filltered_kill_events_timeline_filtered_10up_1.json\n",
      "data/filltered_kill_events_timeline_filtered_10up_2.json\n"
     ]
    }
   ],
   "source": [
    "### OPTION : [6] ì—¬ëŸ¬ ë°ìŠ¤í¬íƒ‘ì—ì„œ replay_scraper ëŒë¦¬ê¸° ìœ„í•œ json íŒŒì¼ ìª¼ê°œê¸°\n",
    "#  ë¶„í• í•˜ì—¬ ì €ì¥í•  ê²½ë¡œ\n",
    "filtered_data_1_path = r\"data/filltered_kill_events_timeline_filtered_10up_1.json\"\n",
    "filtered_data_2_path = r\"data/filltered_kill_events_timeline_filtered_10up_2.json\"\n",
    "\n",
    "# 1) filtered_data.json ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(TOP_KILL_EVENTS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    original = json.load(f)\n",
    "    \n",
    "# ì˜ˆ: { \"data\": { \"7641937851\": [3,5], \"7641940891\": [3,4,6,17], ... } }\n",
    "orig_data = original.get(\"data\", {})\n",
    "\n",
    "# 2) ë‘ ê°œì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ì¤€ë¹„ (keyë“¤ì€ ë™ì¼, valueëŠ” ì ˆë°˜ì”©)\n",
    "data_1 = {}\n",
    "data_2 = {}\n",
    "\n",
    "for match_id, arr in orig_data.items():\n",
    "    if not isinstance(arr, list):\n",
    "        # í˜¹ì‹œ listê°€ ì•„ë‹ˆë©´ ê·¸ëƒ¥ ë‘ ìª½ ë‹¤ ë™ì¼í•˜ê²Œ(ë˜ëŠ” ìŠ¤í‚µ) í•  ìˆ˜ë„ ìˆìŒ\n",
    "        data_1[match_id] = arr\n",
    "        data_2[match_id] = arr\n",
    "        continue\n",
    "    \n",
    "    mid = len(arr) // 2  # ì¤‘ì•™ ì¸ë±ìŠ¤\n",
    "    first_part = arr[:mid]\n",
    "    second_part = arr[mid:]\n",
    "    \n",
    "    data_1[match_id] = first_part\n",
    "    data_2[match_id] = second_part\n",
    "\n",
    "# 3) ìµœì¢… ì €ì¥ êµ¬ì¡°: { \"data\": data_1 }, { \"data\": data_2 }\n",
    "final_1 = {\"data\": data_1}\n",
    "final_2 = {\"data\": data_2}\n",
    "\n",
    "with open(filtered_data_1_path, \"w\", encoding=\"utf-8\") as f1:\n",
    "    json.dump(final_1, f1, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(filtered_data_2_path, \"w\", encoding=\"utf-8\") as f2:\n",
    "    json.dump(final_2, f2, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Done. ë‘ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\\n{filtered_data_1_path}\\n{filtered_data_2_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
