{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:13px\">\n",
    "\n",
    "### 🇰🇷 [KR]\n",
    "\n",
    "---\n",
    "\n",
    "#### 📌 개요\n",
    "\n",
    "각 경기(`matchid`)에 **Pick된 챔피언 목록을 저장**하는 작업입니다.\n",
    "\n",
    "- `.rofl` 리플레이 파일이 아닌  \n",
    "- **CSV 파일**에 있는 `matchid` 목록을 읽어  \n",
    "- 각 경기에 **참여한 챔피언 이름들**을 추출하여 저장합니다.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 있는 리플레이 갯수 :  6893\n"
     ]
    }
   ],
   "source": [
    "import os, json, csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from autoLeague.replays.scraper import ReplayScraper\n",
    "from autoLeague.dataset.riotapi import RiotAPI\n",
    "\n",
    "load_dotenv()  \n",
    "API_KEY = os.environ[\"API_KEY\"]\n",
    "\n",
    "# ── 공통 경로 설정(Set common base path) ─────────────────────────────────────\n",
    "USER_HOME = Path(os.environ[\"USERPROFILE\"])    # C:\\Users\\<계정명>\n",
    "\n",
    "GAME_DIR        = Path(r\"C:\\Riot Games\\League of Legends\\Game\")\n",
    "REPLAY_DIR      = str(USER_HOME / \"Documents\" / \"League of Legends\" / \"Replays\")\n",
    "SAVE_DIR        = str(USER_HOME / \"Desktop\" / \"storage\")\n",
    "SCRAPER_DIR     = str(USER_HOME / \"Desktop\" / \"pyLoL\" / \"pyLoL\" / \"autoLeague\" / \"replays\")\n",
    "MATCH_INFO_DIR  = str(USER_HOME / \"Desktop\" / \"pyLoL\" / \"data\" / \"match_info\")\n",
    "\n",
    "ra = RiotAPI(api_key=API_KEY)\n",
    "rs = ReplayScraper(game_dir=GAME_DIR, replay_dir=REPLAY_DIR, save_dir=SAVE_DIR, scraper_dir=SCRAPER_DIR, replay_speed=27, region=\"KR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:13px\">\n",
    "\n",
    "### 🇰🇷 [KR]\n",
    "\n",
    "---\n",
    "\n",
    "#### 📌 개요\n",
    "\n",
    "이전 단계에서 저장한 **여러 리그의 `matchid` 목록들을 하나의 CSV 파일로 병합**하는 작업입니다.\n",
    "\n",
    "- 다양한 리그에서 수집한 `matchid` CSV들을  \n",
    "- **하나의 통합 CSV 파일로 정리**합니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 🇺🇸 [EN]\n",
    "\n",
    "---\n",
    "\n",
    "#### 📌 Overview\n",
    "\n",
    "This step **merges the `matchid` lists collected from multiple leagues into a single CSV file**.\n",
    "\n",
    "- Combines `matchid` CSV files from various leagues  \n",
    "- into **one consolidated CSV file**.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로 리스트(List of file paths)\n",
    "file_paths = [\n",
    "    \"matchids/matchids_CHALLENGER_I_patch25_13.csv\",\n",
    "    \"matchids/matchids_GRANDMASTER_I_patch25_13.csv\"\n",
    "]\n",
    "\n",
    "# 모든 파일 읽어서 하나로 합치기(Read all files and merge into one DataFrame)\n",
    "df_list = [pd.read_csv(path) for path in file_paths]\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 중복 제거 : match_id 컬럼 기준 (Remove duplicates (based on the 'match_id' column))\n",
    "if 'match_id' in merged_df.columns:\n",
    "    merged_df = merged_df.drop_duplicates(subset='match_id')\n",
    "else:\n",
    "    merged_df = merged_df.drop_duplicates()\n",
    "\n",
    "# 결과 저장(Save the result)\n",
    "merged_df.to_csv(\"data/match_champions(25.13).csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:13px\">\n",
    "\n",
    "### 🇰🇷 [KR]\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔀 OPTION 1: `[2] download_replays.ipynb` 단계를 건너뛴 경우\n",
    "\n",
    "- `.rofl` 리플레이 파일이 아닌  \n",
    "- **CSV 파일에 있는 `matchid` 목록을 읽어**,  \n",
    "- 각 경기에 **참여한 챔피언 이름들을 저장**합니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 🇺🇸 [EN]\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔀 OPTION 1: If you skipped step `[2] download_replays.ipynb`\n",
    "\n",
    "- Instead of using `.rofl` replay files,  \n",
    "- this step **reads `matchid` values from a CSV file** and  \n",
    "- **stores the names of champions who participated** in each match.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.save_champnames_from_matches_without_rofl(input_csv_path='data/match_champions(25.13).csv',\n",
    "                                             output_csv_path='data/match_champions_dict(25.13).csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:13px\">\n",
    "\n",
    "### 🇰🇷 [KR]\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔀 OPTION 2: `[2] download_replays.ipynb` 단계를 완료한 경우\n",
    "\n",
    "- `.rofl` 리플레이 파일들을 **직접 읽어서**,  \n",
    "- 각 경기에 **참여한 챔피언 이름들을 저장**합니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 🇺🇸 [EN]\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔀 OPTION 2: If you completed step `[2] download_replays.ipynb`\n",
    "\n",
    "- Reads the `.rofl` replay files directly  \n",
    "- and **records the champion names** for each match.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.save_champnames_from_matches_with_rofl('data', 'match_champions_dict(25.13).csv', REPLAY_DIR) \n",
    "# 리플레이 파일이 있는 디렉토리 (예시 경로) : replay_dir = r'C:\\Users\\username\\Documents\\League of Legends\\Replays'\n",
    "# CSV를 저장할 디렉토리 : csv_save_folder_dir = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:13px\">\n",
    "\n",
    "### 🇰🇷 [KR]\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧹 STEP 2: 탐지 불가 또는 인게임 초상화가 잦은 챔피언 제외\n",
    "\n",
    "- YOLO-11 기반 **탐지 모델이 제대로 인식하지 못하는 챔피언들**  \n",
    "  (→ 학습 데이터 문제)  \n",
    "- 또는 **패시브 / 스킬로 초상화가 자주 바뀌는 챔피언들**  \n",
    "  (→ 인게임 구조 문제)  \n",
    "을 **데이터셋에서 제거하는 단계**입니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 🇺🇸 [EN]\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧹 STEP 2: Filtering out problematic champion classes\n",
    "\n",
    "- Excludes champions that the **YOLO-11–based model cannot reliably detect**  \n",
    "  (due to training data limitations),  \n",
    "- and champions whose **in-game portraits frequently change**  \n",
    "  (due to passives or skill transformations).  \n",
    "This step filters them **out of the dataset**.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 설정값 ─────────────────────────────────────────────\n",
    "CSV_IN  = \"data/match_champions_dict(25.13).csv\"   # 원본 CSV \n",
    "CSV_OUT = \"data/filtered_matchids(25.13).csv\"              # 결과 저장 경로\n",
    "EXCLUDED_CHAMPIONS = {\"Neeko\", \"Viego\", \"Yuumi\", \"Amumu\",\n",
    "                      \"Kayle\", \"Nilah\", \"Shyvana\"}  # 제외 대상 챔피언\n",
    "# ────────────── ───────────────────────────────────────\n",
    "\n",
    "ra.filter_matches_by_excluded_champions(CSV_IN, CSV_OUT, EXCLUDED_CHAMPIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:13px\">\n",
    "\n",
    "### 🇰🇷 [KR]\n",
    "\n",
    "---\n",
    "\n",
    "#### 🎯 STEP 3 (선택): 10명 평균 점수 기반 상위 경기만 추출\n",
    "\n",
    "- **유저 10명의 평균 점수 기준**으로  \n",
    "- **상위 N개 경기만 필터링**하고 싶다면  \n",
    "- 이 단계를 적용하면 됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 🇺🇸 [EN]\n",
    "\n",
    "---\n",
    "\n",
    "#### 🎯 STEP 3 (Optional): Filter top N matches by average player score\n",
    "\n",
    "- If you want to **select only the top N matches**  \n",
    "- based on the **average score of all 10 players**,  \n",
    "- you can **apply this optional filtering step**.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH_AVGSCR_CSV_PATH = \"data/match_avg_score(25.13).csv\"\n",
    "\n",
    "### [1] match_champ_dict CSV 파일이 있는 경우 사용 > match_info 폴더에 개별적으로 저장.\n",
    "ra.save_summoner_leagueinfo_from_csv(csv_in=CSV_IN, save_folder=MATCH_INFO_DIR, region=\"KR\", queue_type=\"RANKED_SOLO_5x5\")# 기본값queue_type=\"RANKED_SOLO_5x5\"\n",
    "\n",
    "### match_champ_dict CSV 파일이 없는 경우 사용\n",
    "# ra.save_summoner_leagueinfo_of_replays(replay_dir = REPLAY_DIR, save_folder = MATCH_INFO_DIR,region=\"KR\")# 기본값queue_type=\"RANKED_SOLO_5x5\"\n",
    "\n",
    "### [2] 각 경기 유저 10명의 평균 점수 계산하여 csv 에 저장\n",
    "### Calculate the average score of the 10 players in each match and save the results to a CSV file.\n",
    "\n",
    "ra.save_match_avg_score_from_json(match_info_dir=MATCH_INFO_DIR, output_csv_path=MATCH_AVGSCR_CSV_PATH )\n",
    "matchid_list = pd.read_csv(MATCH_AVGSCR_CSV_PATH, usecols=[\"match_id\"]).values.flatten().tolist()\n",
    "\n",
    "### [3] 각 경기에서 각 시간대(분 단위)별로 킬 이벤트가 threshold 이상인 시간대만만 추출하여 json 파일로 저장\n",
    "ra.save_kill_events_from_matchlist(matchids=matchid_list, output_json_path=\"data/kill_events_timeline_from_all_matches.json\", kill_events_threshold=4, max_workers=4) #max_workers는 병렬 처리할 프로세스 수\n",
    "\n",
    "\n",
    "### [4] ── 분(minute) 10 이상만 남기기 ──────────────────────\n",
    "# ── 경로 설정 ────────────────────────────────────────────────\n",
    "INPUT_JSON  = \"data/kill_events_timeline_from_all_matches.json\"\n",
    "OUTPUT_JSON = \"data/kill_events_timeline_filtered_10up.json\"\n",
    "\n",
    "# ── JSON 로드 ───────────────────────────────────────────────\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)          # {\"data\": {match_id: [minutes...] }}\n",
    "\n",
    "# ── 분(minute) 10 이상만 남기기 ────────────────────────────────\n",
    "filtered = {\n",
    "    mid: [m for m in minutes if m >= 10]\n",
    "    for mid, minutes in data.get(\"data\", {}).items()\n",
    "    if any(m >= 10 for m in minutes)       # 10 이상이 하나도 없으면 통째로 제거\n",
    "}\n",
    "\n",
    "# ── 결과 저장 ───────────────────────────────────────────\n",
    "os.makedirs(os.path.dirname(OUTPUT_JSON), exist_ok=True)\n",
    "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"data\": filtered}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"저장 완료 → {OUTPUT_JSON}  (경기 수: {len(filtered)})\")\n",
    "\n",
    "### [5] 상위 N개 경기만 남도록 필터링\n",
    "TOP_KILL_EVENTS_JSON = \"data/top_kill_events_timeline_filtered_10up.json\"\n",
    "ra.save_top_matches_by_score(\n",
    "    avg_score_csv_path=MATCH_AVGSCR_CSV_PATH,\n",
    "    data_json_path=OUTPUT_JSON,\n",
    "    output_path=TOP_KILL_EVENTS_JSON,\n",
    "    top_n=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 두 파일이 생성되었습니다:\n",
      "data/filltered_kill_events_timeline_filtered_10up_1.json\n",
      "data/filltered_kill_events_timeline_filtered_10up_2.json\n"
     ]
    }
   ],
   "source": [
    "### OPTION : [6] 여러 데스크탑에서 replay_scraper 돌리기 위한 json 파일 쪼개기\n",
    "#  분할하여 저장할 경로\n",
    "filtered_data_1_path = r\"data/filltered_kill_events_timeline_filtered_10up_1.json\"\n",
    "filtered_data_2_path = r\"data/filltered_kill_events_timeline_filtered_10up_2.json\"\n",
    "\n",
    "# 1) filtered_data.json 불러오기\n",
    "with open(TOP_KILL_EVENTS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    original = json.load(f)\n",
    "    \n",
    "# 예: { \"data\": { \"7641937851\": [3,5], \"7641940891\": [3,4,6,17], ... } }\n",
    "orig_data = original.get(\"data\", {})\n",
    "\n",
    "# 2) 두 개의 딕셔너리를 준비 (key들은 동일, value는 절반씩)\n",
    "data_1 = {}\n",
    "data_2 = {}\n",
    "\n",
    "for match_id, arr in orig_data.items():\n",
    "    if not isinstance(arr, list):\n",
    "        # 혹시 list가 아니면 그냥 두 쪽 다 동일하게(또는 스킵) 할 수도 있음\n",
    "        data_1[match_id] = arr\n",
    "        data_2[match_id] = arr\n",
    "        continue\n",
    "    \n",
    "    mid = len(arr) // 2  # 중앙 인덱스\n",
    "    first_part = arr[:mid]\n",
    "    second_part = arr[mid:]\n",
    "    \n",
    "    data_1[match_id] = first_part\n",
    "    data_2[match_id] = second_part\n",
    "\n",
    "# 3) 최종 저장 구조: { \"data\": data_1 }, { \"data\": data_2 }\n",
    "final_1 = {\"data\": data_1}\n",
    "final_2 = {\"data\": data_2}\n",
    "\n",
    "with open(filtered_data_1_path, \"w\", encoding=\"utf-8\") as f1:\n",
    "    json.dump(final_1, f1, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(filtered_data_2_path, \"w\", encoding=\"utf-8\") as f2:\n",
    "    json.dump(final_2, f2, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Done. 두 파일이 생성되었습니다:\\n{filtered_data_1_path}\\n{filtered_data_2_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
